{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tviel/work/kaggle_rsna_lumbar_spine/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq /kaggle/input/rsna-abdomen-packages/{pydicom-2.4.3-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n",
    "# !pip install -qqq /kaggle/input/rsna-abdomen-packages/dicomsdl-0.109.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "# !pip install -qqq /kaggle/input/contrails-model-def1/einops-0.6.1-py3-none-any.whl\n",
    "# !pip install -qqq --no-index --find-links /kaggle/input/contrails-wheels/ pretrainedmodels==0.7.4\n",
    "# !pip install -qqq --no-index --find-links /kaggle/input/contrails-wheels/ efficientnet_pytorch==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle/input/rsna-lumbar-spine-code/src\"):\n",
    "    sys.path.append(\"/kaggle/input/rsna-lumbar-spine-code/src\")\n",
    "\n",
    "from util.torch import load_model_weights\n",
    "from util.plots import plot_mask, add_rect\n",
    "from util.metrics import rsna_loss\n",
    "\n",
    "from data.processing import process_and_save\n",
    "from data.transforms import get_transfos\n",
    "from data.dataset import CropDataset, CoordsDataset\n",
    "from data.preparation import prepare_data_crop\n",
    "\n",
    "from inference.seg import get_crops\n",
    "from inference.dataset import ImageInfDataset, FeatureInfDataset\n",
    "from inference.lvl1 import predict, Config\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/timm-smp\"):\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/pytorch-image-models-main/pytorch-image-models-main\"\n",
    "    )\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/segmentation_models.pytorch-master/segmentation_models.pytorch-master\"\n",
    "    )\n",
    "from model_zoo.models import define_model\n",
    "from model_zoo.models_lvl2 import define_model as define_model_2\n",
    "from model_zoo.models_seg import define_model as define_model_seg\n",
    "from model_zoo.models_seg import convert_3d\n",
    "\n",
    "from params import CLASSES_SEG, MODES, LEVELS_, SEVERITIES, LEVELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = True\n",
    "DEBUG = True\n",
    "\n",
    "ROOT_DATA_DIR = \"../input/\"\n",
    "DEBUG_DATA_DIR = \"../output/dataset_debug/\"  # Todo\n",
    "SAVE_FOLDER = \"../output/tmp/\"\n",
    "shutil.rmtree(SAVE_FOLDER)\n",
    "\n",
    "# ROOT_DATA_DIR = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\n",
    "# DEBUG_DATA_DIR = \"/kaggle/input/rsna-2024-debug/\"\n",
    "# SAVE_FOLDER = \"/tmp/\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"npy/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"mid/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"csv/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_DATA_DIR + \"test_images/\"\n",
    "folds_dict = {}\n",
    "\n",
    "if DEBUG:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"train_series_descriptions.csv\")\n",
    "else:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"test_series_descriptions.csv\")\n",
    "\n",
    "df_meta[\"weighting\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[1][:2])\n",
    "df_meta[\"orient\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[0])\n",
    "df_meta[\"study_series\"] = df_meta[\"study_id\"].astype(str) + \"_\" + df_meta[\"series_id\"].astype(str)\n",
    "\n",
    "if DEBUG:\n",
    "    if EVAL:\n",
    "        DATA_PATH = ROOT_DATA_DIR + \"train_images/\"\n",
    "        FOLDS_FILE = DEBUG_DATA_DIR + \"train_folded_v1.csv\"\n",
    "        folds = pd.read_csv(FOLDS_FILE)\n",
    "        df_meta = df_meta.merge(folds, how=\"left\")\n",
    "        df_meta = df_meta[df_meta['fold'] == 1].reset_index(drop=True)\n",
    "    else:\n",
    "        DATA_PATH = DEBUG_DATA_DIR + \"debug_images/\"\n",
    "        df_meta = df_meta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_2 = 512\n",
    "USE_FP16 = True\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "N_JOBS = 16\n",
    "\n",
    "FOLD = 1 if DEBUG else \"fullfit_0\"\n",
    "PLOT = DEBUG and not EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Level 2 model: ../logs/2024-09-04/1/\n",
      "crop ../logs/2024-08-29/5/ ../logs/2024-08-29/5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXP_FOLDERS = {\n",
    "    # \"scs\": (\"../logs/2024-08-04/33/\", [FOLD]),\n",
    "    # \"nfn\": (\"../logs/2024-08-05/27/\", [FOLD]),\n",
    "    # \"ss\": (\"../logs/2024-08-06/17/\", [FOLD]),\n",
    "}\n",
    "\n",
    "COORDS_FOLDERS = {\n",
    "    \"sag\": (\"../logs/2024-08-29/0/\", FOLD),\n",
    "    \"ax\": (\"../logs/2024-09-02/33/\", FOLD),\n",
    "}\n",
    "\n",
    "CROP_EXP_FOLDERS = {\n",
    "    \"crop\": (\"../logs/2024-08-29/5/\", [FOLD], \"crops_0.1\"),\n",
    "}\n",
    "\n",
    "EXP_FOLDERS_2 = [\n",
    "    \"../logs/2024-09-04/1/\",\n",
    "]\n",
    "FOLDS_2 = [FOLD] if DEBUG else [0, 1, 2, 3]\n",
    "\n",
    "EXP_FOLDER_3D = \"../logs/2024-07-31/25/\"\n",
    "\n",
    "for f in EXP_FOLDERS_2:\n",
    "    folders = Config(json.load(open(f + \"config.json\", \"r\"))).exp_folders\n",
    "    print(\"-> Level 2 model:\", f)\n",
    "    for k in folders:\n",
    "        print(k, folders[k], EXP_FOLDERS.get(k, CROP_EXP_FOLDERS.get(k, [\"?\"]))[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>spinal_canal_stenosis_l1_l2</th>\n",
       "      <th>spinal_canal_stenosis_l2_l3</th>\n",
       "      <th>spinal_canal_stenosis_l3_l4</th>\n",
       "      <th>spinal_canal_stenosis_l4_l5</th>\n",
       "      <th>...</th>\n",
       "      <th>left_subarticular_stenosis_l2_l3</th>\n",
       "      <th>left_subarticular_stenosis_l3_l4</th>\n",
       "      <th>left_subarticular_stenosis_l4_l5</th>\n",
       "      <th>left_subarticular_stenosis_l5_s1</th>\n",
       "      <th>right_subarticular_stenosis_l1_l2</th>\n",
       "      <th>right_subarticular_stenosis_l2_l3</th>\n",
       "      <th>right_subarticular_stenosis_l3_l4</th>\n",
       "      <th>right_subarticular_stenosis_l4_l5</th>\n",
       "      <th>right_subarticular_stenosis_l5_s1</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_1054713880</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>2448190387</td>\n",
       "      <td>Axial T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>Axial</td>\n",
       "      <td>4003253_2448190387</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8785691</td>\n",
       "      <td>481125819</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_481125819</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8785691</td>\n",
       "      <td>1570286759</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_1570286759</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description weighting    orient  \\\n",
       "0   4003253   702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "1   4003253  1054713880        Sagittal T1        T1  Sagittal   \n",
       "2   4003253  2448190387           Axial T2        T2     Axial   \n",
       "3   8785691   481125819   Sagittal T2/STIR        T2  Sagittal   \n",
       "4   8785691  1570286759        Sagittal T1        T1  Sagittal   \n",
       "\n",
       "         study_series spinal_canal_stenosis_l1_l2 spinal_canal_stenosis_l2_l3  \\\n",
       "0   4003253_702807833                 Normal/Mild                 Normal/Mild   \n",
       "1  4003253_1054713880                 Normal/Mild                 Normal/Mild   \n",
       "2  4003253_2448190387                 Normal/Mild                 Normal/Mild   \n",
       "3   8785691_481125819                 Normal/Mild                 Normal/Mild   \n",
       "4  8785691_1570286759                 Normal/Mild                 Normal/Mild   \n",
       "\n",
       "  spinal_canal_stenosis_l3_l4 spinal_canal_stenosis_l4_l5  ...  \\\n",
       "0                 Normal/Mild                 Normal/Mild  ...   \n",
       "1                 Normal/Mild                 Normal/Mild  ...   \n",
       "2                 Normal/Mild                 Normal/Mild  ...   \n",
       "3                 Normal/Mild                 Normal/Mild  ...   \n",
       "4                 Normal/Mild                 Normal/Mild  ...   \n",
       "\n",
       "  left_subarticular_stenosis_l2_l3 left_subarticular_stenosis_l3_l4  \\\n",
       "0                      Normal/Mild                      Normal/Mild   \n",
       "1                      Normal/Mild                      Normal/Mild   \n",
       "2                      Normal/Mild                      Normal/Mild   \n",
       "3                      Normal/Mild                      Normal/Mild   \n",
       "4                      Normal/Mild                      Normal/Mild   \n",
       "\n",
       "  left_subarticular_stenosis_l4_l5 left_subarticular_stenosis_l5_s1  \\\n",
       "0                         Moderate                      Normal/Mild   \n",
       "1                         Moderate                      Normal/Mild   \n",
       "2                         Moderate                      Normal/Mild   \n",
       "3                      Normal/Mild                      Normal/Mild   \n",
       "4                      Normal/Mild                      Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l1_l2 right_subarticular_stenosis_l2_l3  \\\n",
       "0                       Normal/Mild                       Normal/Mild   \n",
       "1                       Normal/Mild                       Normal/Mild   \n",
       "2                       Normal/Mild                       Normal/Mild   \n",
       "3                       Normal/Mild                       Normal/Mild   \n",
       "4                       Normal/Mild                       Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l3_l4 right_subarticular_stenosis_l4_l5  \\\n",
       "0                       Normal/Mild                       Normal/Mild   \n",
       "1                       Normal/Mild                       Normal/Mild   \n",
       "2                       Normal/Mild                       Normal/Mild   \n",
       "3                       Normal/Mild                       Normal/Mild   \n",
       "4                       Normal/Mild                       Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l5_s1 fold  \n",
       "0                       Normal/Mild    1  \n",
       "1                       Normal/Mild    1  \n",
       "2                       Normal/Mild    1  \n",
       "3                       Normal/Mild    1  \n",
       "4                       Normal/Mild    1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.logger import upload_to_kaggle\n",
    "\n",
    "# folders = [EXP_FOLDERS[k][0] for k in EXP_FOLDERS]\n",
    "# folders += [CROP_EXP_FOLDERS[k][0] for k in CROP_EXP_FOLDERS]\n",
    "# folders += EXP_FOLDERS_2 + [EXP_FOLDER_3D]\n",
    "\n",
    "# upload_to_kaggle(folders, \"../output/dataset_1/\", \"RSNA 2024 Weights 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bbf836e2de4c52acfdd7e0f85ccaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=N_JOBS)(\n",
    "    delayed(process_and_save)(\n",
    "        df_meta['study_id'][i],\n",
    "        df_meta['series_id'][i],\n",
    "        df_meta['orient'][i],\n",
    "        DATA_PATH,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "        save_meta=True,\n",
    "        save_middle_frame=True,\n",
    "    ) for i in tqdm(range(len(df_meta)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG and not EVAL:\n",
    "    from data.preparation import prepare_data_crop\n",
    "\n",
    "    ref_folder = DEBUG_DATA_DIR + \"npy/\"\n",
    "    # png_ref_folder = \"../input/coords/comp_data/\"\n",
    "\n",
    "    for i in range(len(df_meta)):\n",
    "        study_series = df_meta[\"study_series\"][i]\n",
    "        npy_ref = np.load(ref_folder + f\"{study_series}.npy\")\n",
    "        npy = np.load(SAVE_FOLDER + f\"npy/{study_series}.npy\")\n",
    "        assert (npy == npy_ref).all()\n",
    "\n",
    "        # if df_meta['orient'][i] == \"Axial\":\n",
    "        #     continue\n",
    "\n",
    "        # png_ref = cv2.imread(png_ref_folder + f\"{study_series}.png\")\n",
    "        # png = cv2.imread(SAVE_FOLDER + f\"mid/{study_series}.png\")\n",
    "\n",
    "        # # plt.subplot(1, 2, 1)\n",
    "        # # plt.imshow(png, cmap=\"gray\")\n",
    "        # # plt.subplot(1, 2, 2)\n",
    "        # # plt.imshow(png_ref, cmap=\"gray\")\n",
    "        # # plt.show()\n",
    "        \n",
    "        # assert (png == png_ref).all()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagittal Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>img_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>../output/tmp/mid/4003253_702807833.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  series_id series_description weighting    orient  \\\n",
       "0   4003253  702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "\n",
       "        study_series                                 img_path  \\\n",
       "0  4003253_702807833  ../output/tmp/mid/4003253_702807833.png   \n",
       "\n",
       "                                              target  \n",
       "0  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sag = df_meta[df_meta[\"orient\"] == \"Sagittal\"].reset_index(drop=True)\n",
    "df_sag = df_sag[df_sag.columns[:6]]\n",
    "\n",
    "df_sag['img_path'] = SAVE_FOLDER + \"mid/\" + df_sag[\"study_series\"] + \".png\"\n",
    "df_sag['target'] = [np.ones((5, 2)) for _ in range(len(df_sag))]\n",
    "\n",
    "df_sag.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../logs/2024-08-29/0/coatnet_rmlp_2_rw_384_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_sag = Config(json.load(open(COORDS_FOLDERS['sag'][0] + \"config.json\", \"r\")))\n",
    "\n",
    "model_sag = define_model(\n",
    "    config_sag.name,\n",
    "    drop_rate=config_sag.drop_rate,\n",
    "    drop_path_rate=config_sag.drop_path_rate,\n",
    "    pooling=config_sag.pooling,\n",
    "    num_classes=config_sag.num_classes,\n",
    "    num_classes_aux=config_sag.num_classes_aux,\n",
    "    n_channels=config_sag.n_channels,\n",
    "    reduce_stride=config_sag.reduce_stride,\n",
    "    pretrained=False,\n",
    ")\n",
    "model_sag = model_sag.cuda().eval()\n",
    "\n",
    "weights = COORDS_FOLDERS['sag'][0] + f\"{config_sag.name}_{COORDS_FOLDERS['sag'][1]}.pt\"\n",
    "model_sag = load_model_weights(model_sag, weights, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 s, sys: 763 ms, total: 4.32 s\n",
      "Wall time: 4.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transfos = get_transfos(augment=False, resize=config_sag.resize, use_keypoints=True)\n",
    "dataset = CoordsDataset(df_sag, transforms=transfos)\n",
    "\n",
    "preds_sag, _ = predict(model_sag, dataset, config_sag.loss_config, batch_size=32, use_fp16=True)\n",
    "np.save(SAVE_FOLDER + \"pred_coords_sag.npy\", preds_sag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTAS = [0.1]  #, 0.15]\n",
    "\n",
    "for delta in DELTAS:\n",
    "    os.makedirs(SAVE_FOLDER + f\"crops_{delta}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bf959bfd7c45dcac238cb22e50f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_sag = np.load(SAVE_FOLDER + \"pred_coords_sag.npy\")\n",
    "\n",
    "for idx in tqdm(range(len(df_sag))):\n",
    "    study_series = df_sag[\"study_series\"][idx]\n",
    "    imgs_path = SAVE_FOLDER + \"npy/\" + study_series + \".npy\"\n",
    "\n",
    "    imgs = np.load(imgs_path)\n",
    "    img = imgs[len(imgs) // 2]\n",
    "\n",
    "    preds = preds_sag[idx].reshape(-1, 2).copy()\n",
    "\n",
    "    for delta in DELTAS:  # , 0.15\n",
    "        crops = np.concatenate([preds, preds], -1)\n",
    "        crops[:, [0, 1]] -= delta\n",
    "        crops[:, [2, 3]] += delta\n",
    "        crops = crops.clip(0, 1)\n",
    "\n",
    "        crops[:, [0, 2]] *= imgs.shape[2]\n",
    "        crops[:, [1, 3]] *= imgs.shape[1]\n",
    "        crops = crops.astype(int)\n",
    "\n",
    "        img_crops = []\n",
    "        for i, (x0, y0, x1, y1) in enumerate(crops):\n",
    "            crop = imgs[:, y0: y1, x0: x1].copy()\n",
    "            # assert crop.shape[2] > 1 and crop.shape[1] > 1\n",
    "            np.save(SAVE_FOLDER + f\"crops_{delta}/{study_series}_{LEVELS_[i]}.npy\", crop)\n",
    "            img_crops.append(crop[len(crop) // 2])\n",
    "\n",
    "        if PLOT:\n",
    "            preds[:, 0] *= imgs.shape[2]\n",
    "            preds[:, 1] *= imgs.shape[1]\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(imgs[len(imgs) // 2], cmap=\"gray\")\n",
    "            plt.scatter(preds[:, 0], preds[:, 1], marker=\"x\", label=\"center\")\n",
    "            plt.title(study_series)\n",
    "            plt.axis(False)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "                plt.imshow(img_crops[i], cmap=\"gray\")\n",
    "                plt.axis(False)\n",
    "                plt.title(LEVELS[i])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG and not EVAL:\n",
    "    ref_folder = DEBUG_DATA_DIR + \"coords_crops_0.1/\"\n",
    "    df_ref = prepare_data_crop(ROOT_DATA_DIR, ref_folder).head(10)\n",
    "\n",
    "    df_ref['img_path_2'] = df_ref['img_path'].apply(\n",
    "        lambda x: re.sub(ref_folder, SAVE_FOLDER + f\"crops_0.1/\", x)\n",
    "    )\n",
    "\n",
    "    for i in range(len(df_ref)):\n",
    "        cref = np.load(df_ref['img_path'][i])\n",
    "        c = np.load(df_ref['img_path_2'][i])\n",
    "        assert (cref == c).all()\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.imshow(c[len(c) // 2], cmap=\"gray\")\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.imshow(cref[len(cref) // 2], cmap=\"gray\")\n",
    "        # plt.show()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axial Coords\n",
    "- Not used yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seg & Level 1\n",
    "- Not used currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_seg = Config(json.load(open(EXP_FOLDER_3D + \"config.json\", \"r\")))\n",
    "\n",
    "# model_seg = define_model_seg(\n",
    "#     config_seg.decoder_name,\n",
    "#     config_seg.name,\n",
    "#     num_classes=config_seg.num_classes,\n",
    "#     num_classes_aux=config_seg.num_classes_aux,\n",
    "#     increase_stride=config_seg.increase_stride,\n",
    "#     use_cls=config_seg.use_cls,\n",
    "#     n_channels=config_seg.n_channels,\n",
    "#     use_3d=config_seg.use_3d,\n",
    "#     pretrained=False,\n",
    "# )\n",
    "\n",
    "# model_seg = load_model_weights(\n",
    "#     model_seg, EXP_FOLDER_3D + f\"{config_seg.name}_{FOLD}.pt\"\n",
    "# )\n",
    "# model_seg = model_seg.cuda()\n",
    "# # model_seg = model_seg.eval()  # Hurts results ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "# for mode in EXP_FOLDERS:\n",
    "#     exp_folder, folds = EXP_FOLDERS[mode]\n",
    "#     print(f\"- Mode: {mode}\")\n",
    "#     config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "#     models_ = []\n",
    "#     for fold in folds:\n",
    "#         model = define_model(\n",
    "#             config.name,\n",
    "#             drop_rate=config.drop_rate,\n",
    "#             drop_path_rate=config.drop_path_rate,\n",
    "#             use_gem=config.use_gem,\n",
    "#             num_classes=config.num_classes,\n",
    "#             num_classes_aux=config.num_classes_aux,\n",
    "#             n_channels=config.n_channels,\n",
    "#             reduce_stride=config.reduce_stride,\n",
    "#             increase_stride=(\n",
    "#                 config.increase_stride if hasattr(config, \"increase_stride\") else False\n",
    "#             ),\n",
    "#             pretrained=False,\n",
    "#         )\n",
    "#         model = model.cuda().eval()\n",
    "\n",
    "#         weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "#         model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "#         models_.append(model)\n",
    "\n",
    "#     models[mode] = models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for study in tqdm(sorted(os.listdir(DATA_PATH))):\n",
    "#     if folds_dict.get(int(study), 0) != FOLD and EVAL:\n",
    "#         continue\n",
    "\n",
    "#     for series in sorted(os.listdir(DATA_PATH + study)):\n",
    "#         print(\"\\n-> study\", study, \"- Series\", series)\n",
    "\n",
    "#         imgs, orient, weighting = process(\n",
    "#             study,\n",
    "#             series,\n",
    "#             data_path=DATA_PATH,\n",
    "#             on_gpu=False,\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             weighting, orient = df_meta.loc[(int(study), int(series))].values[1:]\n",
    "#             # print(orient, weighting)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         dfs.append(\n",
    "#             {\n",
    "#                 \"study_id\": study,\n",
    "#                 \"series_id\": series,\n",
    "#                 \"orient\": orient,\n",
    "#                 \"weighting\": weighting,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         print(f\"- Orient {orient} - Weighting {weighting}\")\n",
    "\n",
    "#         # Segmentation\n",
    "#         if orient == \"Sagittal\":\n",
    "#             x = imgs[:, ::-1].copy().astype(np.float32)\n",
    "\n",
    "#             with torch.inference_mode():\n",
    "#                 x = torch.from_numpy(x).cuda()\n",
    "#                 x = F.interpolate(\n",
    "#                     x.unsqueeze(0).unsqueeze(0),\n",
    "#                     config_seg.img_size,\n",
    "#                     mode=\"trilinear\",\n",
    "#                 )\n",
    "#                 x = (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "#                 mask, _ = model_seg(x)\n",
    "#                 mask = F.interpolate(\n",
    "#                     mask,\n",
    "#                     imgs.shape,\n",
    "#                     mode=\"trilinear\",\n",
    "#                 )[\n",
    "#                     0\n",
    "#                 ].argmax(0)\n",
    "#             mask = mask.cpu().numpy()[:, ::-1].astype(np.uint8)\n",
    "\n",
    "#             if DEBUG and PLOT:\n",
    "#                 img_ref = np.load(DEBUG_DATA_DIR + f\"npy/{study}_{series}.npy\")\n",
    "#                 mask_ref = np.load(DEBUG_DATA_DIR + f\"train_segs/{study}_{series}.npy\")\n",
    "#                 delta = (np.abs(mask - mask_ref) > 0).mean()\n",
    "#                 print(\"Mask delta:\", delta)\n",
    "#                 delta = (np.abs(imgs - img_ref) > 0).mean()\n",
    "#                 print(\"Img delta:\", delta)\n",
    "\n",
    "#             if PLOT:\n",
    "#                 f = len(imgs) // 2\n",
    "#                 plt.figure(figsize=(4, 4))\n",
    "#                 plot_mask(imgs[f], mask[f])\n",
    "#                 plt.show()\n",
    "\n",
    "#             # Cropping\n",
    "#             disk_crops = {}\n",
    "#             for disk in CLASSES_SEG[5:]:\n",
    "#                 x0, x1, y0, y1, z0, z1 = get_crops(mask, disk=disk)\n",
    "#                 disk_crops[disk] = (x0, x1, y0, y1, z0, z1)\n",
    "\n",
    "#                 img_crop = imgs[x0:x1, y0:y1, z0:z1]\n",
    "#                 # mask_crop = mask[x0: x1, y0:y1, z0:z1]\n",
    "\n",
    "#                 d = re.sub(\"/\", \"_\", disk.lower())\n",
    "#                 np.save(SAVE_FOLDER + f\"{study}_{series}_{d}.npy\", img_crop.copy())\n",
    "\n",
    "#             if PLOT:\n",
    "#                 plt.figure(figsize=(8, 8))\n",
    "#                 plot_mask(imgs[f], mask[f])\n",
    "\n",
    "#                 for d, disk in enumerate(disk_crops):\n",
    "#                     x0, x1, y0, y1, z0, z1 = disk_crops[disk]\n",
    "#                     add_rect(x0, x1, y0, y1, z0, z1, f, col=\"skyblue\")\n",
    "#                     plt.text(\n",
    "#                         10,\n",
    "#                         (d + 1) * 20,\n",
    "#                         f\"{disk} disk center frame: {int((x1 + x0) / 2)}\",\n",
    "#                         color=\"skyblue\",\n",
    "#                     )\n",
    "#                 plt.show()\n",
    "\n",
    "#         # Cls\n",
    "#         mode = MODES[weighting + \"_\" + orient]\n",
    "#         exp_folder, models_list = EXP_FOLDERS[mode][0], models[mode]\n",
    "\n",
    "#         config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "#         imgs = (imgs - imgs.min()) / (imgs.max() - imgs.min()) * 255\n",
    "#         imgs = imgs.astype(np.uint8)\n",
    "\n",
    "#         transforms = get_transfos(augment=False, resize=config.resize, crop=config.crop)\n",
    "#         dataset = ImageInfDataset(\n",
    "#             imgs,\n",
    "#             transforms=transforms,\n",
    "#             frames_chanel=(\n",
    "#                 config.frames_chanel if hasattr(config, \"frames_chanel\") else 0\n",
    "#             ),\n",
    "#             n_frames=config.n_frames if hasattr(config, \"n_frames\") else 1,\n",
    "#             stride=config.stride if hasattr(config, \"stride\") else 1,\n",
    "#         )\n",
    "\n",
    "#         preds = []\n",
    "#         for model in models_list:\n",
    "#             pred, pred_aux = predict(\n",
    "#                 model,\n",
    "#                 dataset,\n",
    "#                 config.loss_config,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 use_fp16=USE_FP16,\n",
    "#                 num_workers=NUM_WORKERS,\n",
    "#             )\n",
    "#             preds.append(pred)\n",
    "#         preds = np.mean(preds, 0)\n",
    "\n",
    "#         if PLOT:\n",
    "#             plt.figure(figsize=(8, 5))\n",
    "#             plt.plot(preds[:, :, 0])\n",
    "#             plt.show()\n",
    "\n",
    "#         np.save(SAVE_FOLDER + f\"{study}_{series}_{mode}.npy\", preds)\n",
    "\n",
    "# del model_seg, models, imgs, x, mask, dataset\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     for study in sorted(os.listdir(DATA_PATH)):\n",
    "#         for series in sorted(os.listdir(DATA_PATH + study)):\n",
    "#             print(\"-> study\", study, \"- Series\", series)\n",
    "#             for mode in EXP_FOLDERS:\n",
    "#                 exp_folder, folds = EXP_FOLDERS[mode]\n",
    "#                 try:\n",
    "#                     preds_ref = np.load(exp_folder + f\"preds/{study}_{series}.npy\")\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#                 preds = np.load(SAVE_FOLDER + f\"{study}_{series}_{mode}.npy\")\n",
    "\n",
    "#                 assert preds.shape == preds_ref.shape\n",
    "\n",
    "#                 delta = ((preds - preds_ref) ** 2).max()\n",
    "#                 print(f\"{mode} delta :\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     df[\"img_path_ref\"] = DEBUG_DATA_DIR + \"crops_fix/\"\n",
    "#     df[\"img_path_ref\"] += (\n",
    "#         df[\"study_id\"] + \"_\" + df[\"series_id\"] + \"_\" + df[\"level\"] + \".npy\"\n",
    "#     )\n",
    "#     for i in range(len(df)):\n",
    "#         path_ref = df[\"img_path_ref\"][i]\n",
    "#         path = df[\"img_path\"][i]\n",
    "\n",
    "#         if os.path.exists(path_ref):\n",
    "#             crop_ref = np.load(path_ref)\n",
    "#             crop = np.load(path)\n",
    "\n",
    "#             print(\n",
    "#                 f\"Crop {path.split('/')[-1][:-4]} delta:\\t\",\n",
    "#                 ((crop_ref - crop) ** 2).max(),\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_meta.copy()\n",
    "\n",
    "df[\"mode\"] = (df[\"weighting\"] + \"_\" + df[\"orient\"]).map(MODES) + \"_crop\"\n",
    "df[\"target\"] = 0\n",
    "df[\"coords\"] = 0\n",
    "\n",
    "df[\"level\"] = [LEVELS for _ in range(len(df))]\n",
    "df[\"level_\"] = [LEVELS_ for _ in range(len(df))]\n",
    "df = df.explode([\"level\", \"level_\"]).reset_index(drop=True)\n",
    "df[\"img_path_\"] = df[\"study_series\"] + \"_\" + df[\"level_\"] + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7151e79d66384586aae21c125ef17e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model crop - ../logs/2024-08-29/5/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-08-29/5/coatnet_1_rw_224_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crop_fts = {}\n",
    "for mode in tqdm(CROP_EXP_FOLDERS, total=len(CROP_EXP_FOLDERS)):\n",
    "    exp_folder, folds, crop_folder = CROP_EXP_FOLDERS[mode]\n",
    "    print(f\"- Model {mode} - {exp_folder}\")\n",
    "\n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    if mode == \"crop\":\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "    elif \"scs\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode = df_mode[df_mode[\"weighting\"] == \"T2\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "\n",
    "    elif \"nfn\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    elif \"ss\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Axial\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    df_mode['img_path'] = SAVE_FOLDER + crop_folder + \"/\" + df_mode[\"img_path_\"]\n",
    "\n",
    "    transfos = get_transfos(augment=False, resize=config.resize, crop=config.crop)\n",
    "    dataset = CropDataset(\n",
    "        df_mode,\n",
    "        targets=\"target\",\n",
    "        transforms=transfos,\n",
    "        frames_chanel=config.frames_chanel,\n",
    "        n_frames=config.n_frames,\n",
    "        stride=config.stride,\n",
    "        train=False,\n",
    "        load_in_ram=False,\n",
    "    )\n",
    "\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        drop_rate=config.drop_rate,\n",
    "        drop_path_rate=config.drop_path_rate,\n",
    "        pooling=config.pooling,\n",
    "        head_3d=config.head_3d,\n",
    "        n_frames=config.n_frames,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=config.num_classes_aux,\n",
    "        n_channels=config.n_channels,\n",
    "        reduce_stride=config.reduce_stride,\n",
    "        pretrained=False,\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    preds = []\n",
    "    for fold in folds:\n",
    "        weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=1)\n",
    "\n",
    "        pred, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            config.loss_config,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "        preds.append(pred)\n",
    "\n",
    "    preds = np.mean(preds, 0)\n",
    "\n",
    "    if PLOT:\n",
    "        df_ref = pd.read_csv(exp_folder + f\"df_val_{FOLD}.csv\").head(len(preds))\n",
    "        # order_ref = df_ref.sort_values([\"side\", \"level\"]).index.values\n",
    "        preds_ref = np.load(exp_folder + f\"pred_inf_{FOLD}.npy\")[: len(preds)]  # [order_ref]\n",
    "\n",
    "        # plt.figure(figsize=(8, 4))\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(preds)\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(preds_ref)\n",
    "        # plt.show()\n",
    "\n",
    "        delta = (np.abs(preds - preds_ref)).max()\n",
    "        print(preds.shape, preds_ref.shape)\n",
    "        print(f\"{mode} delta:\", delta)\n",
    "\n",
    "    idx = df_mode[[\"study_id\", \"series_id\", \"level\", \"side\"]].values.astype(str).tolist()\n",
    "    idx = [\"_\".join(i) for i in idx]\n",
    "    crop_fts[mode] = dict(zip(idx, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_meta[\n",
    "    [\"study_id\", \"series_id\", \"series_description\"]\n",
    "].groupby('study_id').agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_folder in EXP_FOLDERS_2:\n",
    "    config_2 = Config(json.load(open(exp_folder + \"config.json\", \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureInfDataset(\n",
    "    df_2,\n",
    "    config_2.exp_folders,\n",
    "    crop_fts,\n",
    "    save_folder=SAVE_FOLDER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../logs/2024-09-04/1/simple_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "\n",
    "for exp_folder in EXP_FOLDERS_2:\n",
    "    config_2 = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    dataset = FeatureInfDataset(\n",
    "        df_2,\n",
    "        config_2.exp_folders,\n",
    "        crop_fts,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "    )\n",
    "\n",
    "    model = define_model_2(\n",
    "        config_2.name,\n",
    "        ft_dim=config_2.ft_dim,\n",
    "        layer_dim=config_2.layer_dim,\n",
    "        dense_dim=config_2.dense_dim,\n",
    "        p=config_2.p,\n",
    "        n_fts=config_2.n_fts,\n",
    "        resize=config_2.resize,\n",
    "        num_classes=config_2.num_classes,\n",
    "        num_classes_aux=config_2.num_classes_aux,\n",
    "    )\n",
    "    model = model.eval().cuda()\n",
    "\n",
    "    for fold in FOLDS_2:\n",
    "        weights = exp_folder + f\"{config_2.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "\n",
    "        preds, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            config_2.loss_config,\n",
    "            batch_size=BATCH_SIZE_2,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "        if DEBUG and not EVAL:\n",
    "            preds_ref = np.load(EXP_FOLDERS_2[0] + f\"pred_val_{fold}.npy\")[:1]\n",
    "            delta = np.abs(preds - preds_ref).max()\n",
    "            print(f\"Model {exp_folder} delta:\", delta)\n",
    "\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.997028</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.000955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.992384</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.002051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.969602</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>0.005108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.992068</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.993416</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.988552</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.000720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.453218</td>\n",
       "      <td>0.519617</td>\n",
       "      <td>0.027165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.906213</td>\n",
       "      <td>0.088823</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.993891</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.994281</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.984697</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.000896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.797646</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.007550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.808701</td>\n",
       "      <td>0.172224</td>\n",
       "      <td>0.019075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.006951</td>\n",
       "      <td>0.000923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.988975</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.133831</td>\n",
       "      <td>0.594831</td>\n",
       "      <td>0.271338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.901776</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.011598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.991922</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>0.001203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.988226</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.933793</td>\n",
       "      <td>0.059841</td>\n",
       "      <td>0.006366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.366613</td>\n",
       "      <td>0.543939</td>\n",
       "      <td>0.089448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.924801</td>\n",
       "      <td>0.067683</td>\n",
       "      <td>0.007516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            row_id  normal_mild  moderate  \\\n",
       "0              4003253_spinal_canal_stenosis_l1_l2     0.997743  0.001494   \n",
       "1              4003253_spinal_canal_stenosis_l2_l3     0.997028  0.002017   \n",
       "2              4003253_spinal_canal_stenosis_l3_l4     0.992384  0.005565   \n",
       "3              4003253_spinal_canal_stenosis_l4_l5     0.969602  0.025290   \n",
       "4              4003253_spinal_canal_stenosis_l5_s1     0.997261  0.002006   \n",
       "5    4003253_left_neural_foraminal_narrowing_l1_l2     0.992068  0.007235   \n",
       "6    4003253_left_neural_foraminal_narrowing_l2_l3     0.993416  0.006012   \n",
       "7    4003253_left_neural_foraminal_narrowing_l3_l4     0.988552  0.010727   \n",
       "8    4003253_left_neural_foraminal_narrowing_l4_l5     0.453218  0.519617   \n",
       "9    4003253_left_neural_foraminal_narrowing_l5_s1     0.906213  0.088823   \n",
       "10  4003253_right_neural_foraminal_narrowing_l1_l2     0.993891  0.005519   \n",
       "11  4003253_right_neural_foraminal_narrowing_l2_l3     0.994281  0.005207   \n",
       "12  4003253_right_neural_foraminal_narrowing_l3_l4     0.984697  0.014407   \n",
       "13  4003253_right_neural_foraminal_narrowing_l4_l5     0.797646  0.194805   \n",
       "14  4003253_right_neural_foraminal_narrowing_l5_s1     0.808701  0.172224   \n",
       "15        4003253_left_subarticular_stenosis_l1_l2     0.992126  0.006951   \n",
       "16        4003253_left_subarticular_stenosis_l2_l3     0.988975  0.009743   \n",
       "17        4003253_left_subarticular_stenosis_l3_l4     0.952166  0.043502   \n",
       "18        4003253_left_subarticular_stenosis_l4_l5     0.133831  0.594831   \n",
       "19        4003253_left_subarticular_stenosis_l5_s1     0.901776  0.086625   \n",
       "20       4003253_right_subarticular_stenosis_l1_l2     0.991922  0.006875   \n",
       "21       4003253_right_subarticular_stenosis_l2_l3     0.988226  0.010104   \n",
       "22       4003253_right_subarticular_stenosis_l3_l4     0.933793  0.059841   \n",
       "23       4003253_right_subarticular_stenosis_l4_l5     0.366613  0.543939   \n",
       "24       4003253_right_subarticular_stenosis_l5_s1     0.924801  0.067683   \n",
       "\n",
       "      severe  \n",
       "0   0.000763  \n",
       "1   0.000955  \n",
       "2   0.002051  \n",
       "3   0.005108  \n",
       "4   0.000734  \n",
       "5   0.000698  \n",
       "6   0.000572  \n",
       "7   0.000720  \n",
       "8   0.027165  \n",
       "9   0.004964  \n",
       "10  0.000590  \n",
       "11  0.000512  \n",
       "12  0.000896  \n",
       "13  0.007550  \n",
       "14  0.019075  \n",
       "15  0.000923  \n",
       "16  0.001282  \n",
       "17  0.004333  \n",
       "18  0.271338  \n",
       "19  0.011598  \n",
       "20  0.001203  \n",
       "21  0.001670  \n",
       "22  0.006366  \n",
       "23  0.089448  \n",
       "24  0.007516  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.mean(all_preds, 0).astype(np.float64)\n",
    "studies = dataset.df[[\"study_id\"]].astype(int)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(studies)):\n",
    "    for c, injury in enumerate(config_2.targets):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"row_id\": f'{studies[\"study_id\"][i]}_{injury}',\n",
    "                \"normal_mild\": preds[i, c, 0],\n",
    "                \"moderate\": preds[i, c, 1],\n",
    "                \"severe\": preds[i, c, 2],\n",
    "            }\n",
    "        )\n",
    "\n",
    "sub = pd.DataFrame(rows)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- scs_loss\t: 0.334\n",
      "- nfn_loss\t: 0.507\n",
      "- ss_loss\t: 0.578\n",
      "- any_loss\t: 0.374\n",
      "\n",
      " -> CV Score : 0.448\n"
     ]
    }
   ],
   "source": [
    "if EVAL:\n",
    "    y = pd.read_csv(ROOT_DATA_DIR + \"train.csv\")\n",
    "\n",
    "    for c in y.columns[1:]:\n",
    "        y[c] = y[c].map(dict(zip(SEVERITIES, [0, 1, 2]))).fillna(-1)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    df_val = studies.copy().merge(y, how=\"left\")\n",
    "\n",
    "    avg_loss, losses = rsna_loss(df_val[config_2.targets].values, preds)\n",
    "\n",
    "    for k, v in losses.items():\n",
    "        print(f\"- {k}_loss\\t: {v:.3f}\")\n",
    "\n",
    "    print(f\"\\n -> CV Score : {avg_loss :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
