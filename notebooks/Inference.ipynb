{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tviel/work/kaggle_rsna_lumbar_spine/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq /kaggle/input/rsna-abdomen-packages/{pydicom-2.4.3-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n",
    "# !pip install -qqq /kaggle/input/rsna-abdomen-packages/dicomsdl-0.109.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "# !pip install -qqq /kaggle/input/contrails-model-def1/einops-0.6.1-py3-none-any.whl\n",
    "# !pip install -qqq --no-index --find-links /kaggle/input/contrails-wheels/ pretrainedmodels==0.7.4\n",
    "# !pip install -qqq --no-index --find-links /kaggle/input/contrails-wheels/ efficientnet_pytorch==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle/input/rsna-lumbar-spine-code/src\"):\n",
    "    !cp -r /kaggle/input/rsna-lumbar-spine-code/src ./\n",
    "    sys.path.append(\"src\")\n",
    "\n",
    "from util.torch import load_model_weights\n",
    "from util.plots import plot_mask, add_rect\n",
    "from util.metrics import rsna_loss\n",
    "\n",
    "from data.processing import process_and_save\n",
    "from data.transforms import get_transfos\n",
    "from data.dataset import CropDataset, CoordsDataset\n",
    "from data.preparation import prepare_data_crop\n",
    "\n",
    "from inference.seg import get_crops\n",
    "from inference.dataset import ImageInfDataset, SafeDataset\n",
    "from inference.lvl1 import predict, Config\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/timm-smp\"):\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/pytorch-image-models-main/pytorch-image-models-main\"\n",
    "    )\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/segmentation_models.pytorch-master/segmentation_models.pytorch-master\"\n",
    "    )\n",
    "from model_zoo.models import define_model\n",
    "from model_zoo.models_lvl2 import define_model as define_model_2\n",
    "from model_zoo.models_seg import define_model as define_model_seg\n",
    "from model_zoo.models_seg import convert_3d\n",
    "\n",
    "from params import CLASSES_SEG, MODES, LEVELS_, SEVERITIES, LEVELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = True\n",
    "DEBUG = True\n",
    "\n",
    "ROOT_DATA_DIR = \"../input/\"\n",
    "DEBUG_DATA_DIR = \"../output/dataset_debug/\"  # Todo\n",
    "SAVE_FOLDER = \"../output/tmp/\"\n",
    "shutil.rmtree(SAVE_FOLDER)\n",
    "\n",
    "# ROOT_DATA_DIR = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\n",
    "# DEBUG_DATA_DIR = \"/kaggle/input/rsna-2024-debug/\"\n",
    "# SAVE_FOLDER = \"/tmp/\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"npy/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"mid/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"csv/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_DATA_DIR + \"test_images/\"\n",
    "folds_dict = {}\n",
    "\n",
    "if DEBUG:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"train_series_descriptions.csv\")\n",
    "else:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"test_series_descriptions.csv\")\n",
    "\n",
    "df_meta[\"weighting\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[1][:2])\n",
    "df_meta[\"orient\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[0])\n",
    "df_meta[\"study_series\"] = df_meta[\"study_id\"].astype(str) + \"_\" + df_meta[\"series_id\"].astype(str)\n",
    "\n",
    "if DEBUG:\n",
    "    if EVAL:\n",
    "        DATA_PATH = ROOT_DATA_DIR + \"train_images/\"\n",
    "        FOLDS_FILE = DEBUG_DATA_DIR + \"train_folded_v1.csv\"\n",
    "        folds = pd.read_csv(FOLDS_FILE)\n",
    "        df_meta = df_meta.merge(folds, how=\"left\")\n",
    "        df_meta = df_meta[df_meta['fold'] == 1].reset_index(drop=True)\n",
    "    else:\n",
    "        DATA_PATH = DEBUG_DATA_DIR + \"debug_images/\"\n",
    "        df_meta = df_meta.head(3)\n",
    "\n",
    "        df_meta_ = df_meta.copy()\n",
    "        df_meta_['study_id'] += 1\n",
    "        df_meta_ = df_meta_[df_meta_['orient'] == \"Axial\"]\n",
    "        df_meta = pd.concat([df_meta, df_meta_], ignore_index=True)\n",
    "        df_meta[\"study_series\"] = df_meta[\"study_id\"].astype(str) + \"_\" + df_meta[\"series_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_2 = 512\n",
    "USE_FP16 = True\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "FOLD = 1 if DEBUG else \"fullfit_0\"\n",
    "PLOT = DEBUG and not EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Level 2 model: ../logs/2024-09-04/2/\n",
      "crop ../logs/2024-08-29/5/ ../logs/2024-08-29/5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXP_FOLDERS = {\n",
    "    # \"scs\": (\"../logs/2024-08-04/33/\", [FOLD]),\n",
    "    # \"nfn\": (\"../logs/2024-08-05/27/\", [FOLD]),\n",
    "    # \"ss\": (\"../logs/2024-08-06/17/\", [FOLD]),\n",
    "}\n",
    "\n",
    "COORDS_FOLDERS = {\n",
    "    \"sag\": (\"../logs/2024-08-29/0/\", FOLD),\n",
    "    \"ax\": (\"../logs/2024-09-02/33/\", FOLD),\n",
    "}\n",
    "\n",
    "CROP_EXP_FOLDERS = {\n",
    "    \"crop\": (\"../logs/2024-08-29/5/\", [FOLD], \"crops_0.1\"),\n",
    "}\n",
    "\n",
    "EXP_FOLDERS_2 = [\n",
    "    \"../logs/2024-09-04/2/\",\n",
    "]\n",
    "FOLDS_2 = [FOLD] if DEBUG else [0, 1, 2, 3]\n",
    "\n",
    "EXP_FOLDER_3D = \"../logs/2024-07-31/25/\"\n",
    "\n",
    "for f in EXP_FOLDERS_2:\n",
    "    folders = Config(json.load(open(f + \"config.json\", \"r\"))).exp_folders\n",
    "    print(\"-> Level 2 model:\", f)\n",
    "    for k in folders:\n",
    "        print(k, folders[k], EXP_FOLDERS.get(k, CROP_EXP_FOLDERS.get(k, [\"?\"]))[0])\n",
    "    print()\n",
    "\n",
    "    \n",
    "for k in EXP_FOLDERS:\n",
    "    assert os.path.exists(EXP_FOLDERS[k][0]), f\"Model not found: {k}\"\n",
    "for k in CROP_EXP_FOLDERS:\n",
    "    assert os.path.exists(CROP_EXP_FOLDERS[k][0]), f\"Crop model not found: {k}\"\n",
    "for k in COORDS_FOLDERS:\n",
    "    assert os.path.exists(COORDS_FOLDERS[k][0]), f\"Coords model not found: {k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>spinal_canal_stenosis_l1_l2</th>\n",
       "      <th>spinal_canal_stenosis_l2_l3</th>\n",
       "      <th>spinal_canal_stenosis_l3_l4</th>\n",
       "      <th>spinal_canal_stenosis_l4_l5</th>\n",
       "      <th>...</th>\n",
       "      <th>left_subarticular_stenosis_l2_l3</th>\n",
       "      <th>left_subarticular_stenosis_l3_l4</th>\n",
       "      <th>left_subarticular_stenosis_l4_l5</th>\n",
       "      <th>left_subarticular_stenosis_l5_s1</th>\n",
       "      <th>right_subarticular_stenosis_l1_l2</th>\n",
       "      <th>right_subarticular_stenosis_l2_l3</th>\n",
       "      <th>right_subarticular_stenosis_l3_l4</th>\n",
       "      <th>right_subarticular_stenosis_l4_l5</th>\n",
       "      <th>right_subarticular_stenosis_l5_s1</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_1054713880</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>2448190387</td>\n",
       "      <td>Axial T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>Axial</td>\n",
       "      <td>4003253_2448190387</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8785691</td>\n",
       "      <td>481125819</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_481125819</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8785691</td>\n",
       "      <td>1570286759</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_1570286759</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description weighting    orient  \\\n",
       "0   4003253   702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "1   4003253  1054713880        Sagittal T1        T1  Sagittal   \n",
       "2   4003253  2448190387           Axial T2        T2     Axial   \n",
       "3   8785691   481125819   Sagittal T2/STIR        T2  Sagittal   \n",
       "4   8785691  1570286759        Sagittal T1        T1  Sagittal   \n",
       "\n",
       "         study_series spinal_canal_stenosis_l1_l2 spinal_canal_stenosis_l2_l3  \\\n",
       "0   4003253_702807833                 Normal/Mild                 Normal/Mild   \n",
       "1  4003253_1054713880                 Normal/Mild                 Normal/Mild   \n",
       "2  4003253_2448190387                 Normal/Mild                 Normal/Mild   \n",
       "3   8785691_481125819                 Normal/Mild                 Normal/Mild   \n",
       "4  8785691_1570286759                 Normal/Mild                 Normal/Mild   \n",
       "\n",
       "  spinal_canal_stenosis_l3_l4 spinal_canal_stenosis_l4_l5  ...  \\\n",
       "0                 Normal/Mild                 Normal/Mild  ...   \n",
       "1                 Normal/Mild                 Normal/Mild  ...   \n",
       "2                 Normal/Mild                 Normal/Mild  ...   \n",
       "3                 Normal/Mild                 Normal/Mild  ...   \n",
       "4                 Normal/Mild                 Normal/Mild  ...   \n",
       "\n",
       "  left_subarticular_stenosis_l2_l3 left_subarticular_stenosis_l3_l4  \\\n",
       "0                      Normal/Mild                      Normal/Mild   \n",
       "1                      Normal/Mild                      Normal/Mild   \n",
       "2                      Normal/Mild                      Normal/Mild   \n",
       "3                      Normal/Mild                      Normal/Mild   \n",
       "4                      Normal/Mild                      Normal/Mild   \n",
       "\n",
       "  left_subarticular_stenosis_l4_l5 left_subarticular_stenosis_l5_s1  \\\n",
       "0                         Moderate                      Normal/Mild   \n",
       "1                         Moderate                      Normal/Mild   \n",
       "2                         Moderate                      Normal/Mild   \n",
       "3                      Normal/Mild                      Normal/Mild   \n",
       "4                      Normal/Mild                      Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l1_l2 right_subarticular_stenosis_l2_l3  \\\n",
       "0                       Normal/Mild                       Normal/Mild   \n",
       "1                       Normal/Mild                       Normal/Mild   \n",
       "2                       Normal/Mild                       Normal/Mild   \n",
       "3                       Normal/Mild                       Normal/Mild   \n",
       "4                       Normal/Mild                       Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l3_l4 right_subarticular_stenosis_l4_l5  \\\n",
       "0                       Normal/Mild                       Normal/Mild   \n",
       "1                       Normal/Mild                       Normal/Mild   \n",
       "2                       Normal/Mild                       Normal/Mild   \n",
       "3                       Normal/Mild                       Normal/Mild   \n",
       "4                       Normal/Mild                       Normal/Mild   \n",
       "\n",
       "  right_subarticular_stenosis_l5_s1 fold  \n",
       "0                       Normal/Mild    1  \n",
       "1                       Normal/Mild    1  \n",
       "2                       Normal/Mild    1  \n",
       "3                       Normal/Mild    1  \n",
       "4                       Normal/Mild    1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.logger import upload_to_kaggle\n",
    "\n",
    "# folders = [EXP_FOLDERS[k][0] for k in EXP_FOLDERS]\n",
    "# folders += [CROP_EXP_FOLDERS[k][0] for k in CROP_EXP_FOLDERS]\n",
    "# folders += [COORDS_FOLDERS[k][0] for k in COORDS_FOLDERS]\n",
    "# folders += EXP_FOLDERS_2 + [EXP_FOLDER_3D]\n",
    "# folders = list(set(folders))\n",
    "\n",
    "# upload_to_kaggle(folders, \"../output/dataset_1/\", \"RSNA 2024 Weights 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275ac9d138fd4b26899ccd40f22df62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=NUM_WORKERS)(\n",
    "    delayed(process_and_save)(\n",
    "        df_meta['study_id'][i],\n",
    "        df_meta['series_id'][i],\n",
    "        df_meta['orient'][i],\n",
    "        DATA_PATH,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "        save_meta=False,\n",
    "        save_middle_frame=True,\n",
    "    ) for i in tqdm(range(len(df_meta)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     from data.preparation import prepare_data_crop\n",
    "\n",
    "#     ref_folder = DEBUG_DATA_DIR + \"npy/\"\n",
    "#     # png_ref_folder = \"../input/coords/comp_data/\"\n",
    "\n",
    "#     for i in range(len(df_meta)):\n",
    "#         study_series = df_meta[\"study_series\"][i]\n",
    "#         npy_ref = np.load(ref_folder + f\"{study_series}.npy\")\n",
    "#         npy = np.load(SAVE_FOLDER + f\"npy/{study_series}.npy\")\n",
    "#         assert (npy == npy_ref).all()\n",
    "\n",
    "#         # if df_meta['orient'][i] == \"Axial\":\n",
    "#         #     continue\n",
    "\n",
    "#         # png_ref = cv2.imread(png_ref_folder + f\"{study_series}.png\")\n",
    "#         # png = cv2.imread(SAVE_FOLDER + f\"mid/{study_series}.png\")\n",
    "\n",
    "#         # # plt.subplot(1, 2, 1)\n",
    "#         # # plt.imshow(png, cmap=\"gray\")\n",
    "#         # # plt.subplot(1, 2, 2)\n",
    "#         # # plt.imshow(png_ref, cmap=\"gray\")\n",
    "#         # # plt.show()\n",
    "        \n",
    "#         # assert (png == png_ref).all()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagittal Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>img_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>../output/tmp/mid/4003253_702807833.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_1054713880</td>\n",
       "      <td>../output/tmp/mid/4003253_1054713880.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8785691</td>\n",
       "      <td>481125819</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_481125819</td>\n",
       "      <td>../output/tmp/mid/8785691_481125819.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description weighting    orient  \\\n",
       "0   4003253   702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "1   4003253  1054713880        Sagittal T1        T1  Sagittal   \n",
       "2   8785691   481125819   Sagittal T2/STIR        T2  Sagittal   \n",
       "\n",
       "         study_series                                  img_path  \\\n",
       "0   4003253_702807833   ../output/tmp/mid/4003253_702807833.png   \n",
       "1  4003253_1054713880  ../output/tmp/mid/4003253_1054713880.png   \n",
       "2   8785691_481125819   ../output/tmp/mid/8785691_481125819.png   \n",
       "\n",
       "                                              target  \n",
       "0  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "1  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "2  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  "
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sag = df_meta[df_meta[\"orient\"] == \"Sagittal\"].reset_index(drop=True)\n",
    "df_sag = df_sag[df_sag.columns[:6]]\n",
    "\n",
    "df_sag['img_path'] = SAVE_FOLDER + \"mid/\" + df_sag[\"study_series\"] + \".png\"\n",
    "df_sag['target'] = [np.ones((5, 2)) for _ in range(len(df_sag))]\n",
    "\n",
    "df_sag.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../logs/2024-08-29/0/coatnet_rmlp_2_rw_384_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_sag = Config(json.load(open(COORDS_FOLDERS['sag'][0] + \"config.json\", \"r\")))\n",
    "\n",
    "model_sag = define_model(\n",
    "    config_sag.name,\n",
    "    drop_rate=config_sag.drop_rate,\n",
    "    drop_path_rate=config_sag.drop_path_rate,\n",
    "    pooling=config_sag.pooling,\n",
    "    num_classes=config_sag.num_classes,\n",
    "    num_classes_aux=config_sag.num_classes_aux,\n",
    "    n_channels=config_sag.n_channels,\n",
    "    reduce_stride=config_sag.reduce_stride,\n",
    "    pretrained=False,\n",
    ")\n",
    "model_sag = model_sag.cuda().eval()\n",
    "\n",
    "weights = COORDS_FOLDERS['sag'][0] + f\"{config_sag.name}_{COORDS_FOLDERS['sag'][1]}.pt\"\n",
    "model_sag = load_model_weights(model_sag, weights, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.84 s, sys: 838 ms, total: 4.68 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transfos = get_transfos(augment=False, resize=config_sag.resize, use_keypoints=True)\n",
    "dataset = CoordsDataset(df_sag, transforms=transfos)\n",
    "dataset = SafeDataset(dataset)\n",
    "\n",
    "preds_sag, _ = predict(model_sag, dataset, config_sag.loss_config, batch_size=32, use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTAS = [0.1]  #, 0.15]\n",
    "\n",
    "for delta in DELTAS:\n",
    "    os.makedirs(SAVE_FOLDER + f\"crops_{delta}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80ebef5de094c3698ed57d0a417e6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in tqdm(range(len(df_sag))):\n",
    "    study_series = df_sag[\"study_series\"][idx]\n",
    "    imgs_path = SAVE_FOLDER + \"npy/\" + study_series + \".npy\"\n",
    "\n",
    "    imgs = np.load(imgs_path)\n",
    "\n",
    "    preds = preds_sag[idx].reshape(-1, 2).copy()\n",
    "\n",
    "    for delta in DELTAS:  # , 0.15\n",
    "        crops = np.concatenate([preds, preds], -1)\n",
    "        crops[:, [0, 1]] -= delta\n",
    "        crops[:, [2, 3]] += delta\n",
    "        crops = crops.clip(0, 1)\n",
    "\n",
    "        crops[:, [0, 2]] *= imgs.shape[2]\n",
    "        crops[:, [1, 3]] *= imgs.shape[1]\n",
    "        crops = crops.astype(int)\n",
    "\n",
    "        img_crops = []\n",
    "        for i, (x0, y0, x1, y1) in enumerate(crops):\n",
    "\n",
    "            crop = imgs[:, y0: y1, x0: x1].copy()\n",
    "            # crop = np.zeros((3, 1, 1))\n",
    "            try:\n",
    "                assert crop.shape[2] >= 1 and crop.shape[1] >= 1\n",
    "            except AssertionError:\n",
    "                # print('!!')\n",
    "                # pass\n",
    "                crop = imgs.copy()\n",
    "\n",
    "            np.save(SAVE_FOLDER + f\"crops_{delta}/{study_series}_{LEVELS_[i]}.npy\", crop)\n",
    "            img_crops.append(crop[len(crop) // 2])\n",
    "\n",
    "        if PLOT:\n",
    "            preds[:, 0] *= imgs.shape[2]\n",
    "            preds[:, 1] *= imgs.shape[1]\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(imgs[len(imgs) // 2], cmap=\"gray\")\n",
    "            plt.scatter(preds[:, 0], preds[:, 1], marker=\"x\", label=\"center\")\n",
    "            plt.title(study_series)\n",
    "            plt.axis(False)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "                plt.imshow(img_crops[i], cmap=\"gray\")\n",
    "                plt.axis(False)\n",
    "                plt.title(LEVELS[i])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     ref_folder = DEBUG_DATA_DIR + \"coords_crops_0.1/\"\n",
    "#     df_ref = prepare_data_crop(ROOT_DATA_DIR, ref_folder).head(10)\n",
    "\n",
    "#     df_ref['img_path_2'] = df_ref['img_path'].apply(\n",
    "#         lambda x: re.sub(ref_folder, SAVE_FOLDER + f\"crops_0.1/\", x)\n",
    "#     )\n",
    "\n",
    "#     for i in range(len(df_ref)):\n",
    "#         cref = np.load(df_ref['img_path'][i])\n",
    "#         c = np.load(df_ref['img_path_2'][i])\n",
    "#         assert (cref == c).all()\n",
    "#         # plt.subplot(1, 2, 1)\n",
    "#         # plt.imshow(c[len(c) // 2], cmap=\"gray\")\n",
    "#         # plt.subplot(1, 2, 2)\n",
    "#         # plt.imshow(cref[len(cref) // 2], cmap=\"gray\")\n",
    "#         # plt.show()\n",
    "#         # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axial Coords\n",
    "- Not used yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seg & Level 1\n",
    "- Not used currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_seg = Config(json.load(open(EXP_FOLDER_3D + \"config.json\", \"r\")))\n",
    "\n",
    "# model_seg = define_model_seg(\n",
    "#     config_seg.decoder_name,\n",
    "#     config_seg.name,\n",
    "#     num_classes=config_seg.num_classes,\n",
    "#     num_classes_aux=config_seg.num_classes_aux,\n",
    "#     increase_stride=config_seg.increase_stride,\n",
    "#     use_cls=config_seg.use_cls,\n",
    "#     n_channels=config_seg.n_channels,\n",
    "#     use_3d=config_seg.use_3d,\n",
    "#     pretrained=False,\n",
    "# )\n",
    "\n",
    "# model_seg = load_model_weights(\n",
    "#     model_seg, EXP_FOLDER_3D + f\"{config_seg.name}_{FOLD}.pt\"\n",
    "# )\n",
    "# model_seg = model_seg.cuda()\n",
    "# # model_seg = model_seg.eval()  # Hurts results ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "# for mode in EXP_FOLDERS:\n",
    "#     exp_folder, folds = EXP_FOLDERS[mode]\n",
    "#     print(f\"- Mode: {mode}\")\n",
    "#     config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "#     models_ = []\n",
    "#     for fold in folds:\n",
    "#         model = define_model(\n",
    "#             config.name,\n",
    "#             drop_rate=config.drop_rate,\n",
    "#             drop_path_rate=config.drop_path_rate,\n",
    "#             use_gem=config.use_gem,\n",
    "#             num_classes=config.num_classes,\n",
    "#             num_classes_aux=config.num_classes_aux,\n",
    "#             n_channels=config.n_channels,\n",
    "#             reduce_stride=config.reduce_stride,\n",
    "#             increase_stride=(\n",
    "#                 config.increase_stride if hasattr(config, \"increase_stride\") else False\n",
    "#             ),\n",
    "#             pretrained=False,\n",
    "#         )\n",
    "#         model = model.cuda().eval()\n",
    "\n",
    "#         weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "#         model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "#         models_.append(model)\n",
    "\n",
    "#     models[mode] = models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for study in tqdm(sorted(os.listdir(DATA_PATH))):\n",
    "#     if folds_dict.get(int(study), 0) != FOLD and EVAL:\n",
    "#         continue\n",
    "\n",
    "#     for series in sorted(os.listdir(DATA_PATH + study)):\n",
    "#         print(\"\\n-> study\", study, \"- Series\", series)\n",
    "\n",
    "#         imgs, orient, weighting = process(\n",
    "#             study,\n",
    "#             series,\n",
    "#             data_path=DATA_PATH,\n",
    "#             on_gpu=False,\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             weighting, orient = df_meta.loc[(int(study), int(series))].values[1:]\n",
    "#             # print(orient, weighting)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         dfs.append(\n",
    "#             {\n",
    "#                 \"study_id\": study,\n",
    "#                 \"series_id\": series,\n",
    "#                 \"orient\": orient,\n",
    "#                 \"weighting\": weighting,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         print(f\"- Orient {orient} - Weighting {weighting}\")\n",
    "\n",
    "#         # Segmentation\n",
    "#         if orient == \"Sagittal\":\n",
    "#             x = imgs[:, ::-1].copy().astype(np.float32)\n",
    "\n",
    "#             with torch.inference_mode():\n",
    "#                 x = torch.from_numpy(x).cuda()\n",
    "#                 x = F.interpolate(\n",
    "#                     x.unsqueeze(0).unsqueeze(0),\n",
    "#                     config_seg.img_size,\n",
    "#                     mode=\"trilinear\",\n",
    "#                 )\n",
    "#                 x = (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "#                 mask, _ = model_seg(x)\n",
    "#                 mask = F.interpolate(\n",
    "#                     mask,\n",
    "#                     imgs.shape,\n",
    "#                     mode=\"trilinear\",\n",
    "#                 )[\n",
    "#                     0\n",
    "#                 ].argmax(0)\n",
    "#             mask = mask.cpu().numpy()[:, ::-1].astype(np.uint8)\n",
    "\n",
    "#             if DEBUG and PLOT:\n",
    "#                 img_ref = np.load(DEBUG_DATA_DIR + f\"npy/{study}_{series}.npy\")\n",
    "#                 mask_ref = np.load(DEBUG_DATA_DIR + f\"train_segs/{study}_{series}.npy\")\n",
    "#                 delta = (np.abs(mask - mask_ref) > 0).mean()\n",
    "#                 print(\"Mask delta:\", delta)\n",
    "#                 delta = (np.abs(imgs - img_ref) > 0).mean()\n",
    "#                 print(\"Img delta:\", delta)\n",
    "\n",
    "#             if PLOT:\n",
    "#                 f = len(imgs) // 2\n",
    "#                 plt.figure(figsize=(4, 4))\n",
    "#                 plot_mask(imgs[f], mask[f])\n",
    "#                 plt.show()\n",
    "\n",
    "#             # Cropping\n",
    "#             disk_crops = {}\n",
    "#             for disk in CLASSES_SEG[5:]:\n",
    "#                 x0, x1, y0, y1, z0, z1 = get_crops(mask, disk=disk)\n",
    "#                 disk_crops[disk] = (x0, x1, y0, y1, z0, z1)\n",
    "\n",
    "#                 img_crop = imgs[x0:x1, y0:y1, z0:z1]\n",
    "#                 # mask_crop = mask[x0: x1, y0:y1, z0:z1]\n",
    "\n",
    "#                 d = re.sub(\"/\", \"_\", disk.lower())\n",
    "#                 np.save(SAVE_FOLDER + f\"{study}_{series}_{d}.npy\", img_crop.copy())\n",
    "\n",
    "#             if PLOT:\n",
    "#                 plt.figure(figsize=(8, 8))\n",
    "#                 plot_mask(imgs[f], mask[f])\n",
    "\n",
    "#                 for d, disk in enumerate(disk_crops):\n",
    "#                     x0, x1, y0, y1, z0, z1 = disk_crops[disk]\n",
    "#                     add_rect(x0, x1, y0, y1, z0, z1, f, col=\"skyblue\")\n",
    "#                     plt.text(\n",
    "#                         10,\n",
    "#                         (d + 1) * 20,\n",
    "#                         f\"{disk} disk center frame: {int((x1 + x0) / 2)}\",\n",
    "#                         color=\"skyblue\",\n",
    "#                     )\n",
    "#                 plt.show()\n",
    "\n",
    "#         # Cls\n",
    "#         mode = MODES[weighting + \"_\" + orient]\n",
    "#         exp_folder, models_list = EXP_FOLDERS[mode][0], models[mode]\n",
    "\n",
    "#         config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "#         imgs = (imgs - imgs.min()) / (imgs.max() - imgs.min()) * 255\n",
    "#         imgs = imgs.astype(np.uint8)\n",
    "\n",
    "#         transforms = get_transfos(augment=False, resize=config.resize, crop=config.crop)\n",
    "#         dataset = ImageInfDataset(\n",
    "#             imgs,\n",
    "#             transforms=transforms,\n",
    "#             frames_chanel=(\n",
    "#                 config.frames_chanel if hasattr(config, \"frames_chanel\") else 0\n",
    "#             ),\n",
    "#             n_frames=config.n_frames if hasattr(config, \"n_frames\") else 1,\n",
    "#             stride=config.stride if hasattr(config, \"stride\") else 1,\n",
    "#         )\n",
    "\n",
    "#         preds = []\n",
    "#         for model in models_list:\n",
    "#             pred, pred_aux = predict(\n",
    "#                 model,\n",
    "#                 dataset,\n",
    "#                 config.loss_config,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 use_fp16=USE_FP16,\n",
    "#                 num_workers=NUM_WORKERS,\n",
    "#             )\n",
    "#             preds.append(pred)\n",
    "#         preds = np.mean(preds, 0)\n",
    "\n",
    "#         if PLOT:\n",
    "#             plt.figure(figsize=(8, 5))\n",
    "#             plt.plot(preds[:, :, 0])\n",
    "#             plt.show()\n",
    "\n",
    "#         np.save(SAVE_FOLDER + f\"{study}_{series}_{mode}.npy\", preds)\n",
    "\n",
    "# del model_seg, models, imgs, x, mask, dataset\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     for study in sorted(os.listdir(DATA_PATH)):\n",
    "#         for series in sorted(os.listdir(DATA_PATH + study)):\n",
    "#             print(\"-> study\", study, \"- Series\", series)\n",
    "#             for mode in EXP_FOLDERS:\n",
    "#                 exp_folder, folds = EXP_FOLDERS[mode]\n",
    "#                 try:\n",
    "#                     preds_ref = np.load(exp_folder + f\"preds/{study}_{series}.npy\")\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#                 preds = np.load(SAVE_FOLDER + f\"{study}_{series}_{mode}.npy\")\n",
    "\n",
    "#                 assert preds.shape == preds_ref.shape\n",
    "\n",
    "#                 delta = ((preds - preds_ref) ** 2).max()\n",
    "#                 print(f\"{mode} delta :\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     df[\"img_path_ref\"] = DEBUG_DATA_DIR + \"crops_fix/\"\n",
    "#     df[\"img_path_ref\"] += (\n",
    "#         df[\"study_id\"] + \"_\" + df[\"series_id\"] + \"_\" + df[\"level\"] + \".npy\"\n",
    "#     )\n",
    "#     for i in range(len(df)):\n",
    "#         path_ref = df[\"img_path_ref\"][i]\n",
    "#         path = df[\"img_path\"][i]\n",
    "\n",
    "#         if os.path.exists(path_ref):\n",
    "#             crop_ref = np.load(path_ref)\n",
    "#             crop = np.load(path)\n",
    "\n",
    "#             print(\n",
    "#                 f\"Crop {path.split('/')[-1][:-4]} delta:\\t\",\n",
    "#                 ((crop_ref - crop) ** 2).max(),\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_meta.copy()\n",
    "\n",
    "df[\"target\"] = 0\n",
    "df[\"coords\"] = 0\n",
    "\n",
    "df[\"level\"] = [LEVELS for _ in range(len(df))]\n",
    "df[\"level_\"] = [LEVELS_ for _ in range(len(df))]\n",
    "df = df.explode([\"level\", \"level_\"]).reset_index(drop=True)\n",
    "df[\"img_path_\"] = df[\"study_series\"] + \"_\" + df[\"level_\"] + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ae3920612c4965ac5f217059723806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model crop - ../logs/2024-08-29/5/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-08-29/5/coatnet_1_rw_224_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crop_fts = {}\n",
    "for mode in tqdm(CROP_EXP_FOLDERS, total=len(CROP_EXP_FOLDERS)):\n",
    "    exp_folder, folds, crop_folder = CROP_EXP_FOLDERS[mode]\n",
    "    print(f\"- Model {mode} - {exp_folder}\")\n",
    "\n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    if mode == \"crop\":\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "    elif \"scs\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode = df_mode[df_mode[\"weighting\"] == \"T2\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "    elif \"nfn\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    elif \"ss\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Axial\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    df_mode['img_path'] = SAVE_FOLDER + crop_folder + \"/\" + df_mode[\"img_path_\"]\n",
    "\n",
    "    transfos = get_transfos(augment=False, resize=config.resize, crop=config.crop)\n",
    "    dataset = CropDataset(\n",
    "        df_mode,\n",
    "        targets=\"target\",\n",
    "        transforms=transfos,\n",
    "        frames_chanel=config.frames_chanel,\n",
    "        n_frames=config.n_frames,\n",
    "        stride=config.stride,\n",
    "        train=False,\n",
    "        load_in_ram=False,\n",
    "    )\n",
    "    dataset = SafeDataset(dataset)\n",
    "\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        drop_rate=config.drop_rate,\n",
    "        drop_path_rate=config.drop_path_rate,\n",
    "        pooling=config.pooling,\n",
    "        head_3d=config.head_3d,\n",
    "        n_frames=config.n_frames,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=config.num_classes_aux,\n",
    "        n_channels=config.n_channels,\n",
    "        reduce_stride=config.reduce_stride,\n",
    "        pretrained=False,\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    preds = []\n",
    "    for fold in folds:\n",
    "        weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=1)\n",
    "\n",
    "        pred, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            config.loss_config,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "        preds.append(pred)\n",
    "\n",
    "    preds = np.mean(preds, 0)\n",
    "\n",
    "    if PLOT:\n",
    "        df_ref = pd.read_csv(exp_folder + f\"df_val_{FOLD}.csv\").head(len(preds))\n",
    "        # order_ref = df_ref.sort_values([\"side\", \"level\"]).index.values\n",
    "        preds_ref = np.load(exp_folder + f\"pred_inf_{FOLD}.npy\")[: len(preds)]  # [order_ref]\n",
    "\n",
    "        # plt.figure(figsize=(8, 4))\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(preds)\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(preds_ref)\n",
    "        # plt.show()\n",
    "\n",
    "        delta = (np.abs(preds - preds_ref)).max()\n",
    "        print(preds.shape, preds_ref.shape)\n",
    "        print(f\"{mode} delta:\", delta)\n",
    "\n",
    "    idx = df_mode[[\"study_id\", \"series_id\", \"level\", \"side\"]].values.astype(str).tolist()\n",
    "    idx = [\"_\".join(i) for i in idx]\n",
    "    crop_fts[mode] = dict(zip(idx, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../logs/2024-09-04/2/simple_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_meta[\n",
    "    [\"study_id\", \"series_id\", \"series_description\"]\n",
    "].groupby('study_id').agg(list).reset_index()\n",
    "\n",
    "all_preds = []\n",
    "for exp_folder in EXP_FOLDERS_2:\n",
    "    config_2 = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    dataset = FeatureInfDataset(\n",
    "        df_2,\n",
    "        config_2.exp_folders,\n",
    "        crop_fts,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "    )\n",
    "    dataset = SafeDataset(dataset)\n",
    "\n",
    "    model = define_model_2(\n",
    "        config_2.name,\n",
    "        ft_dim=config_2.ft_dim,\n",
    "        layer_dim=config_2.layer_dim,\n",
    "        dense_dim=config_2.dense_dim,\n",
    "        p=config_2.p,\n",
    "        n_fts=config_2.n_fts,\n",
    "        resize=config_2.resize,\n",
    "        num_classes=config_2.num_classes,\n",
    "        num_classes_aux=config_2.num_classes_aux,\n",
    "    )\n",
    "    model = model.eval().cuda()\n",
    "\n",
    "    for fold in FOLDS_2:\n",
    "        weights = exp_folder + f\"{config_2.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "\n",
    "        preds, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            config_2.loss_config,\n",
    "            batch_size=BATCH_SIZE_2,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "        if DEBUG and not EVAL:\n",
    "            preds_ref = np.load(EXP_FOLDERS_2[0] + f\"pred_val_{fold}.npy\")[:1]\n",
    "            delta = np.abs(preds - preds_ref).max()\n",
    "            print(f\"Model {exp_folder} delta:\", delta)\n",
    "\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>4287160193_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.994340</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.001223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>4287160193_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.912691</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.010239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>4287160193_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>0.005307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4287160193_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.988357</td>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>4287160193_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.994856</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12330</th>\n",
       "      <td>4287160193_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.984932</td>\n",
       "      <td>0.014187</td>\n",
       "      <td>0.000881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>4287160193_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.709698</td>\n",
       "      <td>0.280375</td>\n",
       "      <td>0.009927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>4287160193_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.734184</td>\n",
       "      <td>0.259237</td>\n",
       "      <td>0.006579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>4287160193_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.333449</td>\n",
       "      <td>0.623574</td>\n",
       "      <td>0.042977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>4287160193_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>0.090787</td>\n",
       "      <td>0.007532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>4287160193_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>0.001208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>4287160193_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.889640</td>\n",
       "      <td>0.106304</td>\n",
       "      <td>0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>4287160193_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.489260</td>\n",
       "      <td>0.495028</td>\n",
       "      <td>0.015712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>4287160193_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.404922</td>\n",
       "      <td>0.553733</td>\n",
       "      <td>0.041345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>4287160193_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.919262</td>\n",
       "      <td>0.072753</td>\n",
       "      <td>0.007984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>4287160193_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.003692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341</th>\n",
       "      <td>4287160193_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.376740</td>\n",
       "      <td>0.499039</td>\n",
       "      <td>0.124221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>4287160193_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.581827</td>\n",
       "      <td>0.372823</td>\n",
       "      <td>0.045350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12343</th>\n",
       "      <td>4287160193_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.642496</td>\n",
       "      <td>0.311907</td>\n",
       "      <td>0.045597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12344</th>\n",
       "      <td>4287160193_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.899707</td>\n",
       "      <td>0.084383</td>\n",
       "      <td>0.015909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12345</th>\n",
       "      <td>4287160193_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.957481</td>\n",
       "      <td>0.038304</td>\n",
       "      <td>0.004214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12346</th>\n",
       "      <td>4287160193_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.661213</td>\n",
       "      <td>0.300149</td>\n",
       "      <td>0.038638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>4287160193_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.451243</td>\n",
       "      <td>0.455227</td>\n",
       "      <td>0.093529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>4287160193_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.747769</td>\n",
       "      <td>0.231224</td>\n",
       "      <td>0.021006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>4287160193_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.945184</td>\n",
       "      <td>0.046510</td>\n",
       "      <td>0.008306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  row_id  normal_mild  \\\n",
       "12325             4287160193_spinal_canal_stenosis_l1_l2     0.994340   \n",
       "12326             4287160193_spinal_canal_stenosis_l2_l3     0.912691   \n",
       "12327             4287160193_spinal_canal_stenosis_l3_l4     0.974545   \n",
       "12328             4287160193_spinal_canal_stenosis_l4_l5     0.988357   \n",
       "12329             4287160193_spinal_canal_stenosis_l5_s1     0.994856   \n",
       "12330   4287160193_left_neural_foraminal_narrowing_l1_l2     0.984932   \n",
       "12331   4287160193_left_neural_foraminal_narrowing_l2_l3     0.709698   \n",
       "12332   4287160193_left_neural_foraminal_narrowing_l3_l4     0.734184   \n",
       "12333   4287160193_left_neural_foraminal_narrowing_l4_l5     0.333449   \n",
       "12334   4287160193_left_neural_foraminal_narrowing_l5_s1     0.901681   \n",
       "12335  4287160193_right_neural_foraminal_narrowing_l1_l2     0.980930   \n",
       "12336  4287160193_right_neural_foraminal_narrowing_l2_l3     0.889640   \n",
       "12337  4287160193_right_neural_foraminal_narrowing_l3_l4     0.489260   \n",
       "12338  4287160193_right_neural_foraminal_narrowing_l4_l5     0.404922   \n",
       "12339  4287160193_right_neural_foraminal_narrowing_l5_s1     0.919262   \n",
       "12340        4287160193_left_subarticular_stenosis_l1_l2     0.957912   \n",
       "12341        4287160193_left_subarticular_stenosis_l2_l3     0.376740   \n",
       "12342        4287160193_left_subarticular_stenosis_l3_l4     0.581827   \n",
       "12343        4287160193_left_subarticular_stenosis_l4_l5     0.642496   \n",
       "12344        4287160193_left_subarticular_stenosis_l5_s1     0.899707   \n",
       "12345       4287160193_right_subarticular_stenosis_l1_l2     0.957481   \n",
       "12346       4287160193_right_subarticular_stenosis_l2_l3     0.661213   \n",
       "12347       4287160193_right_subarticular_stenosis_l3_l4     0.451243   \n",
       "12348       4287160193_right_subarticular_stenosis_l4_l5     0.747769   \n",
       "12349       4287160193_right_subarticular_stenosis_l5_s1     0.945184   \n",
       "\n",
       "       moderate    severe  \n",
       "12325  0.004437  0.001223  \n",
       "12326  0.077070  0.010239  \n",
       "12327  0.020147  0.005307  \n",
       "12328  0.009337  0.002306  \n",
       "12329  0.003216  0.001928  \n",
       "12330  0.014187  0.000881  \n",
       "12331  0.280375  0.009927  \n",
       "12332  0.259237  0.006579  \n",
       "12333  0.623574  0.042977  \n",
       "12334  0.090787  0.007532  \n",
       "12335  0.017861  0.001208  \n",
       "12336  0.106304  0.004056  \n",
       "12337  0.495028  0.015712  \n",
       "12338  0.553733  0.041345  \n",
       "12339  0.072753  0.007984  \n",
       "12340  0.038396  0.003692  \n",
       "12341  0.499039  0.124221  \n",
       "12342  0.372823  0.045350  \n",
       "12343  0.311907  0.045597  \n",
       "12344  0.084383  0.015909  \n",
       "12345  0.038304  0.004214  \n",
       "12346  0.300149  0.038638  \n",
       "12347  0.455227  0.093529  \n",
       "12348  0.231224  0.021006  \n",
       "12349  0.046510  0.008306  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.mean(all_preds, 0).astype(np.float64)\n",
    "studies = df_2[[\"study_id\"]].copy().astype(int)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(studies)):\n",
    "    for c, injury in enumerate(config_2.targets):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"row_id\": f'{studies[\"study_id\"].values[i]}_{injury}',\n",
    "                \"normal_mild\": preds[i, c, 0],\n",
    "                \"moderate\": preds[i, c, 1],\n",
    "                \"severe\": preds[i, c, 2],\n",
    "            }\n",
    "        )\n",
    "\n",
    "sub = pd.DataFrame(rows)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- scs_loss\t: 0.334\n",
      "- nfn_loss\t: 0.507\n",
      "- ss_loss\t: 0.578\n",
      "- any_loss\t: 0.371\n",
      "\n",
      " -> CV Score : 0.447\n"
     ]
    }
   ],
   "source": [
    "if EVAL:\n",
    "    y = pd.read_csv(ROOT_DATA_DIR + \"train.csv\")\n",
    "\n",
    "    for c in y.columns[1:]:\n",
    "        y[c] = y[c].map(dict(zip(SEVERITIES, [0, 1, 2]))).fillna(-1)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    df_val = studies.copy().merge(y, how=\"left\")\n",
    "\n",
    "    avg_loss, losses = rsna_loss(df_val[config_2.targets].values, preds)\n",
    "\n",
    "    for k, v in losses.items():\n",
    "        print(f\"- {k}_loss\\t: {v:.3f}\")\n",
    "\n",
    "    print(f\"\\n -> CV Score : {avg_loss :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
