{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_rsna_lumbar_spine/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle/input/rsna-lumbar-spine-code/src\"):\n",
    "    !cp -r /kaggle/input/rsna-lumbar-spine-code/src ./\n",
    "    sys.path.append(\"src\")\n",
    "\n",
    "from util.torch import load_model_weights\n",
    "from util.plots import plot_mask, add_rect\n",
    "from util.metrics import rsna_loss\n",
    "\n",
    "from data.processing import process_and_save\n",
    "from data.transforms import get_transfos\n",
    "from data.dataset import CropDataset, CoordsDataset\n",
    "from data.preparation import prepare_data_crop\n",
    "\n",
    "from inference.seg import get_crops\n",
    "from inference.dataset import ImageInfDataset, FeatureInfDataset, SafeDataset\n",
    "from inference.lvl1 import predict, Config\n",
    "from inference.utils import sub_to_dict\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/timm-smp\"):\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/pytorch-image-models-main/pytorch-image-models-main\"\n",
    "    )\n",
    "    sys.path.append(\n",
    "        \"/kaggle/input/timm-smp/segmentation_models.pytorch-master/segmentation_models.pytorch-master\"\n",
    "    )\n",
    "from model_zoo.models import define_model\n",
    "from model_zoo.models_lvl2 import define_model as define_model_2\n",
    "from model_zoo.models_seg import define_model as define_model_seg\n",
    "from model_zoo.models_seg import convert_3d\n",
    "\n",
    "from params import CLASSES_SEG, MODES, LEVELS_, SEVERITIES, LEVELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = True\n",
    "DEBUG = True\n",
    "\n",
    "ROOT_DATA_DIR = \"../input/\"\n",
    "DEBUG_DATA_DIR = \"../output/dataset_debug/\"  # Todo\n",
    "SAVE_FOLDER = \"../output/tmp/\"\n",
    "shutil.rmtree(SAVE_FOLDER)\n",
    "\n",
    "# ROOT_DATA_DIR = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\n",
    "# DEBUG_DATA_DIR = \"/kaggle/input/rsna-2024-debug/\"\n",
    "# SAVE_FOLDER = \"/tmp/\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"npy/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"mid/\", exist_ok=True)\n",
    "os.makedirs(SAVE_FOLDER + \"csv/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_DATA_DIR + \"test_images/\"\n",
    "folds_dict = {}\n",
    "\n",
    "if DEBUG:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"train_series_descriptions.csv\")\n",
    "else:\n",
    "    df_meta = pd.read_csv(ROOT_DATA_DIR + \"test_series_descriptions.csv\")\n",
    "\n",
    "df_meta[\"weighting\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[1][:2])\n",
    "df_meta[\"orient\"] = df_meta[\"series_description\"].apply(lambda x: x.split()[0])\n",
    "df_meta[\"study_series\"] = df_meta[\"study_id\"].astype(str) + \"_\" + df_meta[\"series_id\"].astype(str)\n",
    "\n",
    "if DEBUG:\n",
    "    if EVAL:\n",
    "        DATA_PATH = ROOT_DATA_DIR + \"train_images/\"\n",
    "        FOLDS_FILE = DEBUG_DATA_DIR + \"train_folded_v1.csv\"\n",
    "        folds = pd.read_csv(FOLDS_FILE)\n",
    "        df_meta = df_meta.merge(folds, how=\"left\")\n",
    "        df_meta = df_meta[df_meta['fold'] == 1].reset_index(drop=True)\n",
    "    else:\n",
    "        DATA_PATH = DEBUG_DATA_DIR + \"debug_images/\"\n",
    "        df_meta = df_meta.head(3)\n",
    "\n",
    "        # df_meta_ = df_meta.copy()\n",
    "        # df_meta_['study_id'] += 1\n",
    "        # df_meta_ = df_meta_[df_meta_['orient'] == \"Axial\"]\n",
    "        # df_meta = pd.concat([df_meta, df_meta_], ignore_index=True)\n",
    "        # df_meta[\"study_series\"] = df_meta[\"study_id\"].astype(str) + \"_\" + df_meta[\"series_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_2 = 512\n",
    "USE_FP16 = True\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "FOLD = 1 if DEBUG else \"fullfit_0\"\n",
    "PLOT = DEBUG and not EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Level 2 model: ../logs/2024-10-04/42/\n",
      "scs_crop_coords ../logs/2024-10-04/34/ ../logs/2024-10-04/34/\n",
      "scs_crop_coords_2 ../logs/2024-10-04/37/ ../logs/2024-10-04/37/\n",
      "crop_2 ../logs/2024-10-04/9/ ../logs/2024-10-04/9/\n",
      "crop ../logs/2024-10-04/1/ ../logs/2024-10-04/1/\n",
      "\n",
      "-> Level 2 model: ../logs/2024-10-04/43/\n",
      "scs_crop_coords ../logs/2024-10-04/34/ ../logs/2024-10-04/34/\n",
      "scs_crop_coords_2 ../logs/2024-10-04/37/ ../logs/2024-10-04/37/\n",
      "dh ../output/oof____cfg_dh_15c.pth ?\n",
      "ch ../output/oof_cfg_ch_35.pth ?\n",
      "crop_2 ../logs/2024-10-04/9/ ../logs/2024-10-04/9/\n",
      "crop ../logs/2024-10-04/1/ ../logs/2024-10-04/1/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXP_FOLDERS = {\n",
    "    # \"scs\": (\"../logs/2024-08-04/33/\", [FOLD]),\n",
    "    # \"nfn\": (\"../logs/2024-08-05/27/\", [FOLD]),\n",
    "    # \"ss\": (\"../logs/2024-08-06/17/\", [FOLD]),\n",
    "}\n",
    "\n",
    "COORDS_FOLDERS = {\n",
    "    \"sag\": (\"../output/2024-08-29_0/\", FOLD),\n",
    "    # \"ax\": (\"../logs/2024-09-02/33/\", FOLD),\n",
    "}\n",
    "\n",
    "# CROP_EXP_FOLDERS = {\n",
    "#     \"crop\": (\"../logs/2024-09-14/5/\", [FOLD], \"crops_0.1\"),\n",
    "#     \"crop_2\": (\"../logs/2024-09-13/7/\", [FOLD], \"crops_0.1\"),\n",
    "#     \"scs_crop_coords\": (\"../output/2024-09-12_1/\", [FOLD], \"crops_0.1\"),  # 5f -0.005 scs\n",
    "#     \"scs_crop_coords_2\": (\"../output/2024-10-02_2/\", [FOLD], \"crops_0.1\"),  # 3f -0.005 scs\n",
    "# }\n",
    "\n",
    "CROP_EXP_FOLDERS = {\n",
    "    \"crop\": (\"../logs/2024-10-04/1/\", [FOLD], \"crops_0.1\"),\n",
    "    \"crop_2\": (\"../logs/2024-10-04/9/\", [FOLD], \"crops_0.1\"),\n",
    "    \"scs_crop_coords\": (\"../logs/2024-10-04/34/\", [FOLD], \"crops_0.1\"),  # 5f -0.005 scs\n",
    "    \"scs_crop_coords_2\": (\"../logs/2024-10-04/37/\", [FOLD], \"crops_0.1\"),  # 3f -0.005 scs\n",
    "}\n",
    "\n",
    "EXP_FOLDERS_2 = [\n",
    "    # \"../output/2024-10-02_3/\",\n",
    "    # \"../logs/2024-10-02/4/\",\n",
    "    # \"../logs/2024-10-02/6/\",\n",
    "    \"../logs/2024-10-04/42/\",  # 0.3943\n",
    "    \"../logs/2024-10-04/43/\",  # 0.3861\n",
    "]\n",
    "FOLDS_2 = [FOLD] if DEBUG else [0, 1, 2, 3]\n",
    "\n",
    "# EXP_FOLDER_3D = \"../logs/2024-07-31/25/\"\n",
    "\n",
    "for f in EXP_FOLDERS_2:\n",
    "    folders = Config(json.load(open(f + \"config.json\", \"r\"))).exp_folders\n",
    "    print(\"-> Level 2 model:\", f)\n",
    "    for k in folders:\n",
    "        print(k, folders[k], EXP_FOLDERS.get(k, CROP_EXP_FOLDERS.get(k, [\"?\"]))[0])\n",
    "    print()\n",
    "\n",
    "    \n",
    "for k in EXP_FOLDERS:\n",
    "    assert os.path.exists(EXP_FOLDERS[k][0]), f\"Model not found: {k}\"\n",
    "for k in CROP_EXP_FOLDERS:\n",
    "    assert os.path.exists(CROP_EXP_FOLDERS[k][0]), f\"Crop model not found: {k}\"\n",
    "for k in COORDS_FOLDERS:\n",
    "    assert os.path.exists(COORDS_FOLDERS[k][0]), f\"Coords model not found: {k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_1054713880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>2448190387</td>\n",
       "      <td>Axial T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>Axial</td>\n",
       "      <td>4003253_2448190387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8785691</td>\n",
       "      <td>481125819</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_481125819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8785691</td>\n",
       "      <td>1570286759</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_1570286759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description weighting    orient  \\\n",
       "0   4003253   702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "1   4003253  1054713880        Sagittal T1        T1  Sagittal   \n",
       "2   4003253  2448190387           Axial T2        T2     Axial   \n",
       "3   8785691   481125819   Sagittal T2/STIR        T2  Sagittal   \n",
       "4   8785691  1570286759        Sagittal T1        T1  Sagittal   \n",
       "\n",
       "         study_series  fold  \n",
       "0   4003253_702807833     1  \n",
       "1  4003253_1054713880     1  \n",
       "2  4003253_2448190387     1  \n",
       "3   8785691_481125819     1  \n",
       "4  8785691_1570286759     1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Copying ../logs/2024-10-04/1/ ...\n",
      "- Copying ../logs/2024-10-04/42/ ...\n",
      "- Copying ../logs/2024-10-04/37/ ...\n",
      "- Copying ../logs/2024-10-04/9/ ...\n",
      "- Copying ../logs/2024-10-04/34/ ...\n",
      "- Copying ../logs/2024-10-04/43/ ...\n",
      "\n",
      "Dataset size : 7.842 Go\n",
      "- Update existing dataset !\n",
      "- Uploading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807M/807M [00:04<00:00, 190MB/s]  \n",
      "100%|██████████| 1.31G/1.31G [00:06<00:00, 224MB/s] \n",
      "100%|██████████| 4.33M/4.33M [00:00<00:00, 9.91MB/s]\n",
      "100%|██████████| 753M/753M [00:05<00:00, 149MB/s] \n",
      "100%|██████████| 22.0/22.0 [00:00<00:00, 62.4B/s]\n",
      "100%|██████████| 3.35M/3.35M [00:00<00:00, 8.62MB/s]\n",
      "100%|██████████| 753M/753M [00:05<00:00, 145MB/s]  \n",
      "100%|██████████| 4.66M/4.66M [00:00<00:00, 11.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "from util.logger import upload_to_kaggle\n",
    "\n",
    "folders = [EXP_FOLDERS[k][0] for k in EXP_FOLDERS]\n",
    "folders += [CROP_EXP_FOLDERS[k][0] for k in CROP_EXP_FOLDERS]\n",
    "# folders += [COORDS_FOLDERS[k][0] for k in COORDS_FOLDERS]\n",
    "folders += EXP_FOLDERS_2 # + [EXP_FOLDER_3D]\n",
    "folders = list(set(folders))\n",
    "\n",
    "upload_to_kaggle(folders, \"../output/dataset_2/\", \"RSNA 2024 Weights 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038d67aa4ef640718a837fba88a2e834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=NUM_WORKERS)(\n",
    "    delayed(process_and_save)(\n",
    "        df_meta['study_id'][i],\n",
    "        df_meta['series_id'][i],\n",
    "        df_meta['orient'][i],\n",
    "        DATA_PATH,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "        save_meta=False,\n",
    "        save_middle_frame=True,\n",
    "    ) for i in tqdm(range(len(df_meta)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG and not EVAL:\n",
    "#     from data.preparation import prepare_data_crop\n",
    "\n",
    "#     ref_folder = DEBUG_DATA_DIR + \"npy/\"\n",
    "#     # png_ref_folder = \"../input/coords/comp_data/\"\n",
    "\n",
    "#     for i in range(len(df_meta)):\n",
    "#         study_series = df_meta[\"study_series\"][i]\n",
    "#         npy_ref = np.load(ref_folder + f\"{study_series}.npy\")\n",
    "#         npy = np.load(SAVE_FOLDER + f\"npy/{study_series}.npy\")\n",
    "#         assert (npy == npy_ref).all()\n",
    "\n",
    "#         # if df_meta['orient'][i] == \"Axial\":\n",
    "#         #     continue\n",
    "\n",
    "#         # png_ref = cv2.imread(png_ref_folder + f\"{study_series}.png\")\n",
    "#         # png = cv2.imread(SAVE_FOLDER + f\"mid/{study_series}.png\")\n",
    "\n",
    "#         # # plt.subplot(1, 2, 1)\n",
    "#         # # plt.imshow(png, cmap=\"gray\")\n",
    "#         # # plt.subplot(1, 2, 2)\n",
    "#         # # plt.imshow(png_ref, cmap=\"gray\")\n",
    "#         # # plt.show()\n",
    "        \n",
    "#         # assert (png == png_ref).all()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagittal Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>weighting</th>\n",
       "      <th>orient</th>\n",
       "      <th>study_series</th>\n",
       "      <th>img_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_702807833</td>\n",
       "      <td>../output/tmp/mid/4003253_702807833.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>4003253_1054713880</td>\n",
       "      <td>../output/tmp/mid/4003253_1054713880.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8785691</td>\n",
       "      <td>481125819</td>\n",
       "      <td>Sagittal T2/STIR</td>\n",
       "      <td>T2</td>\n",
       "      <td>Sagittal</td>\n",
       "      <td>8785691_481125819</td>\n",
       "      <td>../output/tmp/mid/8785691_481125819.png</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description weighting    orient  \\\n",
       "0   4003253   702807833   Sagittal T2/STIR        T2  Sagittal   \n",
       "1   4003253  1054713880        Sagittal T1        T1  Sagittal   \n",
       "2   8785691   481125819   Sagittal T2/STIR        T2  Sagittal   \n",
       "\n",
       "         study_series                                  img_path  \\\n",
       "0   4003253_702807833   ../output/tmp/mid/4003253_702807833.png   \n",
       "1  4003253_1054713880  ../output/tmp/mid/4003253_1054713880.png   \n",
       "2   8785691_481125819   ../output/tmp/mid/8785691_481125819.png   \n",
       "\n",
       "                                              target  \n",
       "0  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "1  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "2  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sag = df_meta[df_meta[\"orient\"] == \"Sagittal\"].reset_index(drop=True)\n",
    "df_sag = df_sag[df_sag.columns[:6]]\n",
    "\n",
    "df_sag['img_path'] = SAVE_FOLDER + \"mid/\" + df_sag[\"study_series\"] + \".png\"\n",
    "df_sag['target'] = [np.ones((5, 2)) for _ in range(len(df_sag))]\n",
    "\n",
    "df_sag.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../output/2024-08-29_0/coatnet_rmlp_2_rw_384_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_sag = Config(json.load(open(COORDS_FOLDERS['sag'][0] + \"config.json\", \"r\")))\n",
    "\n",
    "model_sag = define_model(\n",
    "    config_sag.name,\n",
    "    drop_rate=config_sag.drop_rate,\n",
    "    drop_path_rate=config_sag.drop_path_rate,\n",
    "    pooling=config_sag.pooling,\n",
    "    num_classes=config_sag.num_classes,\n",
    "    num_classes_aux=config_sag.num_classes_aux,\n",
    "    n_channels=config_sag.n_channels,\n",
    "    reduce_stride=config_sag.reduce_stride,\n",
    "    pretrained=False,\n",
    ")\n",
    "model_sag = model_sag.cuda().eval()\n",
    "\n",
    "weights = COORDS_FOLDERS['sag'][0] + f\"{config_sag.name}_{COORDS_FOLDERS['sag'][1]}.pt\"\n",
    "model_sag = load_model_weights(model_sag, weights, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.65 s, sys: 3.76 s, total: 10.4 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transfos = get_transfos(augment=False, resize=config_sag.resize, use_keypoints=True)\n",
    "dataset = CoordsDataset(df_sag, transforms=transfos)\n",
    "dataset = SafeDataset(dataset)\n",
    "\n",
    "preds_sag, _ = predict(model_sag, dataset, config_sag.loss_config, batch_size=32, use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTAS = [0.1]  #, 0.15]\n",
    "\n",
    "for delta in DELTAS:\n",
    "    os.makedirs(SAVE_FOLDER + f\"crops_{delta}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a57d1bebdf948efb8913504e9f98f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in tqdm(range(len(df_sag))):\n",
    "    study_series = df_sag[\"study_series\"][idx]\n",
    "    imgs_path = SAVE_FOLDER + \"npy/\" + study_series + \".npy\"\n",
    "\n",
    "    imgs = np.load(imgs_path)\n",
    "\n",
    "    preds = preds_sag[idx].reshape(-1, 2).copy()\n",
    "\n",
    "    for delta in DELTAS:  # , 0.15\n",
    "        crops = np.concatenate([preds, preds], -1)\n",
    "        crops[:, [0, 1]] -= delta\n",
    "        crops[:, [2, 3]] += delta\n",
    "        crops = crops.clip(0, 1)\n",
    "\n",
    "        crops[:, [0, 2]] *= imgs.shape[2]\n",
    "        crops[:, [1, 3]] *= imgs.shape[1]\n",
    "        crops = crops.astype(int)\n",
    "\n",
    "        img_crops = []\n",
    "        for i, (x0, y0, x1, y1) in enumerate(crops):\n",
    "\n",
    "            crop = imgs[:, y0: y1, x0: x1].copy()\n",
    "            # crop = np.zeros((3, 1, 1))\n",
    "            try:\n",
    "                assert crop.shape[2] >= 1 and crop.shape[1] >= 1\n",
    "            except AssertionError:\n",
    "                # print('!!')\n",
    "                # pass\n",
    "                crop = imgs.copy()\n",
    "\n",
    "            np.save(SAVE_FOLDER + f\"crops_{delta}/{study_series}_{LEVELS_[i]}.npy\", crop)\n",
    "            img_crops.append(crop[len(crop) // 2])\n",
    "\n",
    "        if PLOT:\n",
    "            preds[:, 0] *= imgs.shape[2]\n",
    "            preds[:, 1] *= imgs.shape[1]\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(imgs[len(imgs) // 2], cmap=\"gray\")\n",
    "            plt.scatter(preds[:, 0], preds[:, 1], marker=\"x\", label=\"center\")\n",
    "            plt.title(study_series)\n",
    "            plt.axis(False)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "                plt.imshow(img_crops[i], cmap=\"gray\")\n",
    "                plt.axis(False)\n",
    "                plt.title(LEVELS[i])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG and not EVAL:\n",
    "    ref_folder = DEBUG_DATA_DIR + \"coords_crops_0.1_2/\"\n",
    "    df_ref = prepare_data_crop(ROOT_DATA_DIR, ref_folder).head(10)\n",
    "\n",
    "    df_ref['img_path_2'] = df_ref['img_path'].apply(\n",
    "        lambda x: re.sub(ref_folder, SAVE_FOLDER + f\"crops_0.1/\", x)\n",
    "    )\n",
    "\n",
    "    for i in range(len(df_ref)):\n",
    "        cref = np.load(df_ref['img_path'][i])\n",
    "        c = np.load(df_ref['img_path_2'][i])\n",
    "        assert (cref == c).all()\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.imshow(c[len(c) // 2], cmap=\"gray\")\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.imshow(cref[len(cref) // 2], cmap=\"gray\")\n",
    "        # plt.show()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_meta.copy()\n",
    "\n",
    "df[\"target\"] = 0\n",
    "df[\"coords\"] = 0\n",
    "\n",
    "df[\"level\"] = [LEVELS for _ in range(len(df))]\n",
    "df[\"level_\"] = [LEVELS_ for _ in range(len(df))]\n",
    "df = df.explode([\"level\", \"level_\"]).reset_index(drop=True)\n",
    "df[\"img_path_\"] = df[\"study_series\"] + \"_\" + df[\"level_\"] + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db38e77a6b44e38b0f122521c8df4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model crop - ../logs/2024-10-04/1/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-10-04/1/coatnet_1_rw_224_1.pt\n",
      "\n",
      "- Model crop_2 - ../logs/2024-10-04/9/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-10-04/9/coatnet_1_rw_224_1.pt\n",
      "\n",
      "- Model scs_crop_coords - ../logs/2024-10-04/34/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-10-04/34/coatnet_2_rw_224_1.pt\n",
      "\n",
      "- Model scs_crop_coords_2 - ../logs/2024-10-04/37/\n",
      "\n",
      " -> Loading encoder weights from ../logs/2024-10-04/37/coatnet_1_rw_224_1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crop_fts = {}\n",
    "for mode in tqdm(CROP_EXP_FOLDERS, total=len(CROP_EXP_FOLDERS)):\n",
    "    exp_folder, folds, crop_folder = CROP_EXP_FOLDERS[mode]\n",
    "    print(f\"- Model {mode} - {exp_folder}\")\n",
    "\n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    if mode in [\"crop\", \"crop_2\"]:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "    elif \"scs\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode = df_mode[df_mode[\"weighting\"] == \"T2\"].reset_index(drop=True)\n",
    "        df_mode[\"side\"] = \"Center\"\n",
    "    elif \"nfn\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Sagittal\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    elif \"ss\" in mode:\n",
    "        df_mode = df[df['orient'] == \"Axial\"]\n",
    "        df_mode[\"side\"] = [\"Right\", \"Left\"]\n",
    "        df_mode = df_mode.explode(\"side\").reset_index(drop=True)\n",
    "        df_mode = df_mode.sort_values(\n",
    "            [\"study_id\", \"series_id\", \"side\", \"level\"],\n",
    "            ascending=[True, True, False, True],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    df_mode['img_path'] = SAVE_FOLDER + crop_folder + \"/\" + df_mode[\"img_path_\"]\n",
    "\n",
    "    transfos = get_transfos(augment=False, resize=config.resize, crop=config.crop)\n",
    "    dataset = CropDataset(\n",
    "        df_mode,\n",
    "        targets=\"target\",\n",
    "        transforms=transfos,\n",
    "        frames_chanel=config.frames_chanel,\n",
    "        n_frames=config.n_frames,\n",
    "        stride=config.stride,\n",
    "        train=False,\n",
    "        load_in_ram=False,\n",
    "    )\n",
    "    dataset = SafeDataset(dataset)\n",
    "\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        drop_rate=config.drop_rate,\n",
    "        drop_path_rate=config.drop_path_rate,\n",
    "        pooling=config.pooling,\n",
    "        head_3d=config.head_3d,\n",
    "        n_frames=config.n_frames,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=config.num_classes_aux,\n",
    "        n_channels=config.n_channels,\n",
    "        reduce_stride=config.reduce_stride,\n",
    "        pretrained=False,\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    if mode == \"crop_2\":\n",
    "        model.delta = 1\n",
    "\n",
    "    preds = []\n",
    "    for fold in folds:\n",
    "        weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=1)\n",
    "\n",
    "        pred, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            config.loss_config,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "        preds.append(pred)\n",
    "\n",
    "    preds = np.mean(preds, 0)\n",
    "\n",
    "    if PLOT:\n",
    "        df_ref = pd.read_csv(exp_folder + f\"df_val_{FOLD}.csv\").head(len(preds))\n",
    "        # order_ref = df_ref.sort_values([\"side\", \"level\"]).index.values\n",
    "        preds_ref = np.load(exp_folder + f\"pred_inf_{FOLD}.npy\")[: len(preds)]  # [order_ref]\n",
    "\n",
    "        # plt.figure(figsize=(8, 4))\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(preds)\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(preds_ref)\n",
    "        # plt.show()\n",
    "\n",
    "        delta = (np.abs(preds - preds_ref)).max()\n",
    "        print(preds.shape, preds_ref.shape)\n",
    "        print(f\"{mode} delta:\", delta)\n",
    "\n",
    "    idx = df_mode[[\"study_id\", \"series_id\", \"level\", \"side\"]].values.astype(str).tolist()\n",
    "    idx = [\"_\".join(i) for i in idx]\n",
    "    crop_fts[mode] = dict(zip(idx, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_fts = {\n",
    "#     \"ch\": sub_to_dict(\"submission.csv\"),\n",
    "#     \"dh\": sub_to_dict(\"submission.csv\"),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_fts = {}\n",
    "# for k in ['ch', 'dh']:\n",
    "#     # config_2.exp_folders['dh'], config_2.exp_folders['ch']\n",
    "#     file = torch.load(config_2.exp_folders[k])\n",
    "#     csv_fts[k] = dict(zip(\n",
    "#         file[\"study_id\"].tolist(),\n",
    "#         file['logits'].float().cpu().numpy(),\n",
    "#     ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_SCS = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading encoder weights from ../logs/2024-10-04/42/simple_1.pt\n",
      "\n",
      "0.06163797\n",
      "0.078248516\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_meta[\n",
    "    [\"study_id\", \"series_id\", \"series_description\"]\n",
    "].groupby('study_id').agg(list).reset_index()\n",
    "\n",
    "all_preds = []\n",
    "for exp_folder in EXP_FOLDERS_2:\n",
    "    config_2 = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    # LOCAL\n",
    "    csv_fts = {}\n",
    "    # for k in config_2.exp_folders:\n",
    "    #     if \"ch\" in k or \"dh\" in k:\n",
    "    #         file = torch.load(config_2.exp_folders[k])\n",
    "    #         csv_fts[k] = dict(zip(\n",
    "    #             file[\"study_id\"].tolist(),\n",
    "    #             file['logits'].float().cpu().numpy(),\n",
    "    #         ))\n",
    "\n",
    "    dataset = FeatureInfDataset(\n",
    "        df_2,\n",
    "        config_2.exp_folders,\n",
    "        crop_fts,\n",
    "        csv_fts,\n",
    "        save_folder=SAVE_FOLDER,\n",
    "    )\n",
    "    dataset = SafeDataset(dataset)\n",
    "\n",
    "    model = define_model_2(\n",
    "        config_2.name,\n",
    "        ft_dim=config_2.ft_dim,\n",
    "        layer_dim=config_2.layer_dim,\n",
    "        dense_dim=config_2.dense_dim,\n",
    "        p=config_2.p,\n",
    "        n_fts=config_2.n_fts,\n",
    "        resize=config_2.resize,\n",
    "        num_classes=config_2.num_classes,\n",
    "        num_classes_aux=config_2.num_classes_aux,\n",
    "    )\n",
    "    model = model.eval().cuda()\n",
    "\n",
    "    for fold in FOLDS_2:\n",
    "        weights = exp_folder + f\"{config_2.name}_{fold}.pt\"\n",
    "        model = load_model_weights(model, weights, verbose=config_2.local_rank == 0)\n",
    "\n",
    "        preds, _ = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            {\"activation\": \"\"},\n",
    "            batch_size=BATCH_SIZE_2,\n",
    "            use_fp16=USE_FP16,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "        preds[:, :5, 0] += DELTA_SCS[0]\n",
    "        preds[:, :5, 1] += DELTA_SCS[1]\n",
    "        preds[:, :5, 2] += DELTA_SCS[2]\n",
    "\n",
    "        preds = softmax(preds, axis=-1)\n",
    "\n",
    "        print(preds[:, :5, 2].mean())\n",
    "        print(preds[:, :5, 1].mean())\n",
    "\n",
    "        if DEBUG and not EVAL:\n",
    "            preds_ref = np.load(EXP_FOLDERS_2[0] + f\"pred_val_{fold}.npy\")[:1]\n",
    "            delta = np.abs(preds - preds_ref).max()\n",
    "            print(f\"Model {exp_folder} delta:\", delta)\n",
    "\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98449087e-01, 1.15544372e-03, 3.95430136e-04],\n",
       "       [9.98531461e-01, 1.09190773e-03, 3.76616896e-04],\n",
       "       [9.98351932e-01, 1.27384928e-03, 3.74349242e-04],\n",
       "       [9.94846702e-01, 4.31105401e-03, 8.42290639e-04],\n",
       "       [9.97571886e-01, 1.81617111e-03, 6.11916068e-04],\n",
       "       [9.90095377e-01, 9.45684966e-03, 4.47817001e-04],\n",
       "       [9.91010070e-01, 8.61904025e-03, 3.70871247e-04],\n",
       "       [9.84841466e-01, 1.45116448e-02, 6.46847708e-04],\n",
       "       [5.26179612e-01, 4.55370486e-01, 1.84498597e-02],\n",
       "       [8.77769232e-01, 1.16609305e-01, 5.62152220e-03],\n",
       "       [9.87190545e-01, 1.22073805e-02, 6.02010346e-04],\n",
       "       [9.88296092e-01, 1.11832255e-02, 5.20562287e-04],\n",
       "       [9.80690837e-01, 1.85276289e-02, 7.81524752e-04],\n",
       "       [7.04198360e-01, 2.85356641e-01, 1.04449913e-02],\n",
       "       [9.34114397e-01, 6.25817701e-02, 3.30378138e-03],\n",
       "       [9.93528545e-01, 5.65926498e-03, 8.12115846e-04],\n",
       "       [9.92886722e-01, 6.23578252e-03, 8.77539394e-04],\n",
       "       [9.84289706e-01, 1.38359703e-02, 1.87432475e-03],\n",
       "       [1.75371855e-01, 6.26317739e-01, 1.98310375e-01],\n",
       "       [9.21888590e-01, 7.18997717e-02, 6.21163612e-03],\n",
       "       [9.91864085e-01, 7.24031124e-03, 8.95673293e-04],\n",
       "       [9.91931021e-01, 7.21257087e-03, 8.56386207e-04],\n",
       "       [9.74730790e-01, 2.32106764e-02, 2.05855654e-03],\n",
       "       [5.24052441e-01, 4.32338536e-01, 4.36090045e-02],\n",
       "       [9.51543629e-01, 4.37288210e-02, 4.72750748e-03]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.998449</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.998531</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.998352</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.994847</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.000842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4003253_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.997572</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.990095</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.991010</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.984841</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.526180</td>\n",
       "      <td>0.455370</td>\n",
       "      <td>0.018450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.877769</td>\n",
       "      <td>0.116609</td>\n",
       "      <td>0.005622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.987191</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.988296</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.000521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.980691</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.704198</td>\n",
       "      <td>0.285357</td>\n",
       "      <td>0.010445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>0.062582</td>\n",
       "      <td>0.003304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.993529</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>0.000812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.992887</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.000878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.984290</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.175372</td>\n",
       "      <td>0.626318</td>\n",
       "      <td>0.198310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4003253_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.921889</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.006212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.991864</td>\n",
       "      <td>0.007240</td>\n",
       "      <td>0.000896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.991931</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0.000856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.974731</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>0.002059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.524052</td>\n",
       "      <td>0.432339</td>\n",
       "      <td>0.043609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4003253_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.951544</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.004728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            row_id  normal_mild  moderate  \\\n",
       "0              4003253_spinal_canal_stenosis_l1_l2     0.998449  0.001155   \n",
       "1              4003253_spinal_canal_stenosis_l2_l3     0.998531  0.001092   \n",
       "2              4003253_spinal_canal_stenosis_l3_l4     0.998352  0.001274   \n",
       "3              4003253_spinal_canal_stenosis_l4_l5     0.994847  0.004311   \n",
       "4              4003253_spinal_canal_stenosis_l5_s1     0.997572  0.001816   \n",
       "5    4003253_left_neural_foraminal_narrowing_l1_l2     0.990095  0.009457   \n",
       "6    4003253_left_neural_foraminal_narrowing_l2_l3     0.991010  0.008619   \n",
       "7    4003253_left_neural_foraminal_narrowing_l3_l4     0.984841  0.014512   \n",
       "8    4003253_left_neural_foraminal_narrowing_l4_l5     0.526180  0.455370   \n",
       "9    4003253_left_neural_foraminal_narrowing_l5_s1     0.877769  0.116609   \n",
       "10  4003253_right_neural_foraminal_narrowing_l1_l2     0.987191  0.012207   \n",
       "11  4003253_right_neural_foraminal_narrowing_l2_l3     0.988296  0.011183   \n",
       "12  4003253_right_neural_foraminal_narrowing_l3_l4     0.980691  0.018528   \n",
       "13  4003253_right_neural_foraminal_narrowing_l4_l5     0.704198  0.285357   \n",
       "14  4003253_right_neural_foraminal_narrowing_l5_s1     0.934114  0.062582   \n",
       "15        4003253_left_subarticular_stenosis_l1_l2     0.993529  0.005659   \n",
       "16        4003253_left_subarticular_stenosis_l2_l3     0.992887  0.006236   \n",
       "17        4003253_left_subarticular_stenosis_l3_l4     0.984290  0.013836   \n",
       "18        4003253_left_subarticular_stenosis_l4_l5     0.175372  0.626318   \n",
       "19        4003253_left_subarticular_stenosis_l5_s1     0.921889  0.071900   \n",
       "20       4003253_right_subarticular_stenosis_l1_l2     0.991864  0.007240   \n",
       "21       4003253_right_subarticular_stenosis_l2_l3     0.991931  0.007213   \n",
       "22       4003253_right_subarticular_stenosis_l3_l4     0.974731  0.023211   \n",
       "23       4003253_right_subarticular_stenosis_l4_l5     0.524052  0.432339   \n",
       "24       4003253_right_subarticular_stenosis_l5_s1     0.951544  0.043729   \n",
       "\n",
       "      severe  \n",
       "0   0.000395  \n",
       "1   0.000377  \n",
       "2   0.000374  \n",
       "3   0.000842  \n",
       "4   0.000612  \n",
       "5   0.000448  \n",
       "6   0.000371  \n",
       "7   0.000647  \n",
       "8   0.018450  \n",
       "9   0.005622  \n",
       "10  0.000602  \n",
       "11  0.000521  \n",
       "12  0.000782  \n",
       "13  0.010445  \n",
       "14  0.003304  \n",
       "15  0.000812  \n",
       "16  0.000878  \n",
       "17  0.001874  \n",
       "18  0.198310  \n",
       "19  0.006212  \n",
       "20  0.000896  \n",
       "21  0.000856  \n",
       "22  0.002059  \n",
       "23  0.043609  \n",
       "24  0.004728  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.mean(all_preds, 0).astype(np.float64)\n",
    "studies = df_2[[\"study_id\"]].copy().astype(int)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(studies)):\n",
    "    for c, injury in enumerate(config_2.targets):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"row_id\": f'{studies[\"study_id\"].values[i]}_{injury}',\n",
    "                \"normal_mild\": preds[i, c, 0],\n",
    "                \"moderate\": preds[i, c, 1],\n",
    "                \"severe\": preds[i, c, 2],\n",
    "            }\n",
    "        )\n",
    "\n",
    "sub = pd.DataFrame(rows)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt any mean 0.223\n",
      "pred any mean 0.219\n",
      "\n",
      "- scs_loss\t: 0.312\n",
      "- nfn_loss\t: 0.482\n",
      "- ss_loss\t: 0.556\n",
      "- any_loss\t: 0.358\n",
      "\n",
      " -> CV Score : 0.427\n"
     ]
    }
   ],
   "source": [
    "if EVAL:\n",
    "    y = pd.read_csv(ROOT_DATA_DIR + \"train.csv\")\n",
    "\n",
    "    for c in y.columns[1:]:\n",
    "        y[c] = y[c].map(dict(zip(SEVERITIES, [0, 1, 2]))).fillna(-1)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    df_val = studies.copy().merge(y, how=\"left\")\n",
    "\n",
    "    avg_loss, losses = rsna_loss(df_val[config_2.targets].values, preds, verbose=1)\n",
    "\n",
    "    for k, v in losses.items():\n",
    "        print(f\"- {k}_loss\\t: {v:.3f}\")\n",
    "\n",
    "    print(f\"\\n -> CV Score : {avg_loss :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
