{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fefbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5d4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tviel/work/kaggle_rsna_lumbar_spine/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tviel/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e6ad4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-12T16:03:00.516834Z",
     "iopub.status.busy": "2023-08-12T16:03:00.516342Z",
     "iopub.status.idle": "2023-08-12T16:03:02.824246Z",
     "shell.execute_reply": "2023-08-12T16:03:02.823119Z"
    },
    "papermill": {
     "duration": 2.317669,
     "end_time": "2023-08-12T16:03:02.827155",
     "exception": false,
     "start_time": "2023-08-12T16:03:00.509486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d620e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "\n",
    "from data.dataset import *\n",
    "from data.transforms import *\n",
    "from data.preparation import *\n",
    "from data.processing import read_series_metadata\n",
    "from data.sagittal_to_axial import get_axial_coords\n",
    "\n",
    "from model_zoo.models import define_model\n",
    "from util.torch import load_model_weights\n",
    "from util.plots import *\n",
    "\n",
    "from inference.lvl1 import Config, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0980a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_crop(DATA_PATH, crop_folder=\"../input/crops_0.15_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357cb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prepare_data_nfn(DATA_PATH, crop_folder=\"../input/crops_0.15_2/\")\n",
    "# df = df[df['side'] == 'Left']\n",
    "# df_preds_coords = pd.read_csv('../output/seg_sag_coords.csv')\n",
    "# df = df.merge(df_preds_coords, how=\"left\")\n",
    "# delta = df.apply(lambda x: x.left - x.coords[0] if x.side == \"Left\" else x.right - x.coords[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca0ec4",
   "metadata": {},
   "source": [
    "## Sagittal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eccccf",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acf7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2024-08-28/13/\"  # coatnet_rmlp_2_rw_384 50 ep new folds\n",
    "EXP_FOLDER = \"../logs/2024-08-28/24/\"  # coatnet_rmlp_2_rw_384 aug 50 ep new folds\n",
    "EXP_FOLDER = \"../logs/2024-08-29/0/\"  # coatnet_rmlp_2_rw_384 aug 50 ep new folds\n",
    "FOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea1d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = pd.read_csv(DATA_PATH + 'train_label_coordinates_v0.csv')\n",
    "# coords = coords[coords['condition'] == \"Spinal Canal Stenosis\"].sort_values([\"study_id\", \"series_id\", 'level'])\n",
    "# coords = coords.groupby([\"study_id\", \"series_id\"]).agg(list).reset_index()\n",
    "# coords = coords[coords['level'].apply(len) != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcadd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "# df_y[df_y.isna().values[:, :15].sum(1) > 0]\n",
    "# os.listdir('../input/train_images/2492114990/')\n",
    "# df_gt = prepare_data_scs(explode=False)\n",
    "# df_gt = df_gt[df_gt[\"level\"].apply(lambda x: 0 if isinstance(x, float) else len(x)) != 5]\n",
    "# df_gt.head(1)\n",
    "\n",
    "# for i, row in df_gt.iterrows():\n",
    "#     try:\n",
    "#         missing = [j for j, r in enumerate(LEVELS)  if r not in row.level]\n",
    "#     except:\n",
    "#         missing = range(5)\n",
    "\n",
    "#     for i in missing:\n",
    "#         print(\"Missing disk\", LEVELS[i], \" - target\", row[CLASSES_SCS[i]])\n",
    "#     print()\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617fcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "\n",
    "df = prepare_coords_data(config.coords_folder, use_ext=config.use_ext)\n",
    "\n",
    "folds = pd.read_csv(config.folds_file)\n",
    "df = df.merge(folds, how=\"left\")\n",
    "df['fold'] = df['fold'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903f63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models_sag = []\n",
    "# for fold in range(4):\n",
    "#     model = define_model(\n",
    "#         config.name,\n",
    "#         drop_rate=config.drop_rate,\n",
    "#         drop_path_rate=config.drop_path_rate,\n",
    "#         pooling=config.pooling,\n",
    "#         num_classes=config.num_classes,\n",
    "#         num_classes_aux=config.num_classes_aux,\n",
    "#         n_channels=config.n_channels,\n",
    "#         reduce_stride=config.reduce_stride,\n",
    "#         pretrained=False,\n",
    "#     )\n",
    "#     model = model.cuda().eval()\n",
    "\n",
    "#     weights = EXP_FOLDER + f\"{config.name}_{fold}.pt\"\n",
    "#     try:\n",
    "#         model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "#     except FileNotFoundError:\n",
    "#         continue\n",
    "#     # models_sag.append(model)\n",
    "\n",
    "#     df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "#     dataset = CoordsDataset(df_val, transforms=get_transfos(augment=False, resize=config.resize, use_keypoints=True))\n",
    "\n",
    "#     preds, _ = predict(model, dataset, config.loss_config, batch_size=32, use_fp16=True)\n",
    "\n",
    "#     np.save(EXP_FOLDER + f\"pred_inf_{fold}.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce921ddc",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76563826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = pd.read_csv('../input/train_label_coordinates.csv')\n",
    "\n",
    "# cc = cc.merge(\n",
    "#     cc[[\"series_id\", \"level\"]].groupby(\"series_id\").count().reset_index(),\n",
    "#     how=\"left\",\n",
    "#     on=\"series_id\",\n",
    "#     suffixes=(\"\", \"_count\"),\n",
    "# )\n",
    "\n",
    "# dfg = (\n",
    "#     cc[(cc.condition.isin([\"Right Neural Foraminal Narrowing\", \"Left Neural Foraminal Narrowing\"])) & ~(cc.level_count.isin([10]))]\n",
    "#     .groupby([\"study_id\", \"series_id\"])\n",
    "#     .agg(list)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# dfg.shape\n",
    "\n",
    "# # dfg = (\n",
    "# #     cc[(cc.condition == \"Spinal Canal Stenosis\") & (cc.level_count != 5)]\n",
    "# #     .sort_values(\"level\")\n",
    "# #     .groupby([\"study_id\", \"series_id\"])\n",
    "# #     .agg(list)\n",
    "# #     .reset_index()\n",
    "# # )\n",
    "# # dfg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2354a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO_FIX = [\n",
    "#     # 1468566581,\n",
    "#     # 2185202709,\n",
    "#     # 716946645,\n",
    "#     # 3123114360,\n",
    "#     # 2143604834,\n",
    "#     # 1540250849,\n",
    "#     # 2943022937,\n",
    "#     # 425681838,\n",
    "#     # 1638921810,\n",
    "#     # 4089223112,\n",
    "#     # 648725109,\n",
    "#     # 2177693773,\n",
    "#     # 2929396535,\n",
    "#     # 4245678886,\n",
    "#     # 2538968579,\n",
    "# ]\n",
    "\n",
    "# def shift_disk_up():\n",
    "#     df = pd.read_csv('../input/train_label_coordinates.csv')\n",
    "\n",
    "#     for series in TO_FIX:\n",
    "#         df_fixes = df.loc[df['series_id'] == series].sort_values('level').reset_index(drop=True)\n",
    "#         df = df[df['series_id'] != series].reset_index(drop=True)\n",
    "\n",
    "#         for _, df_fix  in df_fixes.groupby('condition'):\n",
    "#             df_fix['level'] = df_fix['level'].map({'L1/L2': \"L2/L3\", 'L2/L3': \"L3/L4\", 'L3/L4': \"L4/L5\", 'L4/L5': \"L5/S1\"})\n",
    "#             df_fix = df_fix.dropna(axis=0)\n",
    "#             df = pd.concat([df, df_fix], ignore_index=True)\n",
    "\n",
    "#     df.to_csv('../input/train_label_coordinates_shift.csv')\n",
    "\n",
    "# # shift_disk_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db78c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = prepare_data()\n",
    "df_sev = prepare_data_crop(DATA_PATH)\n",
    "df_spinenet = pd.read_csv('../output/spinenet_kps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98402b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDIES = df_gt[df_gt['series_id'].isin(TO_FIX)].study_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c879d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_predictions(p, y, verbose=0):\n",
    "    d = np.abs(p - y) * 100\n",
    "    d = d[y.sum(-1) > 0].mean()\n",
    "\n",
    "    if y.min() > 0 and d > 1:\n",
    "        return y\n",
    "\n",
    "    delta = (np.random.random() - 0.5) / 100\n",
    "    \n",
    "    # Shift down\n",
    "    p_down = p[1:]\n",
    "    y_down = y[:-1]\n",
    "    d_down = np.abs(p_down - y_down) * 100\n",
    "    d_down = d_down[y_down.sum(-1) > 0].mean()\n",
    "\n",
    "    # Shift up\n",
    "    p_up = p[:-1]\n",
    "    y_up = y[1:]\n",
    "    d_up = np.abs(p_up - y_up) * 100\n",
    "    d_up = d_up[y_up.sum(-1) > 0].mean()\n",
    "\n",
    "    if d_up < d:  # shift up\n",
    "        if verbose:\n",
    "            print('Shift up')\n",
    "        if y[0].sum() > 0:\n",
    "            fix = y[0] + delta\n",
    "        else:\n",
    "            fix = p[0] + np.array([0.03, -0.09])\n",
    "        return np.vstack([fix, p_up])\n",
    "\n",
    "    elif d_down < d:  # shift down\n",
    "        if verbose:\n",
    "            print('Shift down')\n",
    "        if y[-1].sum() > 0:\n",
    "            fix = y[-1] + delta\n",
    "        else:\n",
    "            fix = p[-1] + np.array([0.04, 0.09])\n",
    "        return np.vstack([p_down, fix])\n",
    "\n",
    "    else:\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbea599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6d59f8f3c043089b25c7e7fc79130e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = []\n",
    "df_ = prepare_data()\n",
    "\n",
    "for fold in range(4):\n",
    "    preds = np.load(EXP_FOLDER + f\"pred_inf_{fold}.npy\")\n",
    "    df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    dataset = CoordsDataset(df_val, transforms=get_transfos(augment=False, use_keypoints=True))\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        study = df_val['study_id'][idx]\n",
    "        series = df_val['series_id'][idx]\n",
    "\n",
    "        # if df_val['series_description'][idx] != \"Sagittal T2/STIR\":\n",
    "        #     continue\n",
    "\n",
    "        # if series in TO_FIX:\n",
    "        #     continue\n",
    "        # if not study in STUDIES:\n",
    "        #     continue\n",
    "\n",
    "        img, y, _ = dataset[idx]\n",
    "        labels = np.vstack(df_sev[df_sev['series_id'] == series].sort_values('level')['target'].values)\n",
    "\n",
    "        gt = df_gt[df_gt['series_id'] == series]\n",
    "        imgs = np.load(f'../input/npy2/{study}_{series}.npy')\n",
    "\n",
    "        # print(gt['coords'].values[0][:, 0])\n",
    "        # frame = int(np.round(gt['coords'].values[0][:, 0].mean()))\n",
    "        frame = len(imgs) // 2\n",
    "        # frame = len(imgs) // 4\n",
    "        # frame = int(gt['coords'].values[0][-1, 0])\n",
    "\n",
    "        img = imgs[frame]\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "        try:\n",
    "            spinenet_coords = df_spinenet[df_spinenet['series_id'] == series]\n",
    "            spinenet_coords = spinenet_coords.values[len(spinenet_coords)  // 2, -10:].reshape(2, 5).T\n",
    "            spinenet_coords[:, 0] /= img.shape[1]\n",
    "            spinenet_coords[:, 0] /= img.shape[0]\n",
    "            # p = spinenet_coords.copy()\n",
    "            # p[:, 0] /= img.shape[1]\n",
    "            # p[:, 1] /= img.shape[0]\n",
    "        except:\n",
    "            spinenet_coords = None\n",
    "\n",
    "        p_ = preds[idx].reshape(-1, 2)\n",
    "        p = preds[idx].reshape(-1, 2)\n",
    "\n",
    "        # p = fix_predictions(p, y.numpy())\n",
    "        p_ = p.copy()\n",
    "\n",
    "        d = np.abs(p - y.numpy()) * 100\n",
    "        d = d[y.sum(-1) > 0].mean()\n",
    "        ds.append(d)\n",
    "\n",
    "        if PLOT:\n",
    "            if d > 3:\n",
    "                y = y[y.sum(-1) > 0]\n",
    "                # if len(y) == 5:\n",
    "                #     continue\n",
    "\n",
    "                print(study, series)\n",
    "                print('SCS / L-NFN / R-NFN / L-SS / R-SS')\n",
    "                print(labels)\n",
    "\n",
    "                # cv2.imwrite(f'../output/fix/{study}_{series}.png', (img * 255).astype(np.uint8))\n",
    "\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                plt.scatter(y[:, 0] * img.shape[1], y[:, 1] * img.shape[0], marker=\"x\", label=\"truth\")\n",
    "                plt.scatter(p_[:, 0] * img.shape[1], p_[:, 1] * img.shape[0], marker=\"x\", label=\"pred\")\n",
    "                if spinenet_coords is not None:\n",
    "                    plt.scatter(spinenet_coords[:, 0], spinenet_coords[:, 1], marker=\"x\", label=\"spinenet\")\n",
    "                plt.title(f'Dist = {d:.2f} - study {study} - series {series}')\n",
    "                plt.axis(False)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                print(p_[:, 0] * img.shape[1], p_[:, 1] * img.shape[0])\n",
    "\n",
    "            # if idx > 50:\n",
    "                # break\n",
    "    # if PLOT:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c1d6ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with error > 5%: 6\n",
      "Images with error > 4%: 9\n",
      "Images with error > 3%: 21\n"
     ]
    }
   ],
   "source": [
    "print('Images with error > 5%:', (np.array(ds) > 5).sum())\n",
    "print('Images with error > 4%:', (np.array(ds) > 4).sum())\n",
    "print('Images with error > 3%:', (np.array(ds) > 3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc35bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(ds)\n",
    "plt.axvline(np.mean(ds), c=\"salmon\")\n",
    "plt.text(np.mean(ds), 100, f\"   mean={np.mean(ds):.3f}\", color=\"salmon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64045e0",
   "metadata": {},
   "source": [
    "### Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f88900",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 0.1\n",
    "\n",
    "SAVE = True\n",
    "PLOT = False\n",
    "\n",
    "SAVE_FOLDER = f\"../input/coords_crops_{DELTA}_2/\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "020f0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(4):\n",
    "#     pred_val = np.load(EXP_FOLDER + f\"pred_inf_{fold}.npy\")\n",
    "#     df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "#     for idx in tqdm(range(len(df_val))):\n",
    "#         study_series = df_val[\"img_path\"][idx].split('/')[-1][:-4]\n",
    "#         imgs_path = DATA_PATH + \"npy2/\" + study_series + \".npy\"      ###### NPY2 ??\n",
    "#         imgs = np.load(imgs_path)\n",
    "#         img = imgs[0]\n",
    "\n",
    "#         # try:\n",
    "#         #     series = df_val[\"series_id\"][idx]\n",
    "#         #     spinenet_coords = df_spinenet[df_spinenet['series_id'] == series]\n",
    "#         #     spinenet_coords = spinenet_coords.values[len(spinenet_coords)  // 2, -10:].reshape(2, 5).T\n",
    "#         #     spinenet_coords[:, 0] /= img.shape[1]\n",
    "#         #     spinenet_coords[:, 1] /= img.shape[0]\n",
    "#         #     preds = spinenet_coords.copy()\n",
    "#         # except IndexError:\n",
    "#         #     print('No Spinenet coords found')\n",
    "#         preds = pred_val[idx].reshape(-1, 2)\n",
    "#         # preds = fix_predictions(preds, df_val[\"target_rel\"][idx], verbose=1)\n",
    "\n",
    "#         assert preds.min() >= 0\n",
    "#         assert preds.max() <= 1\n",
    "\n",
    "#         crops = np.concatenate([preds, preds], -1)\n",
    "#         crops[:, [0, 1]] -= DELTA\n",
    "#         crops[:, [2, 3]] += DELTA\n",
    "#         crops = crops.clip(0, 1)\n",
    "#         crops[:, [0, 2]] *= imgs.shape[2]\n",
    "#         crops[:, [1, 3]] *= imgs.shape[1]\n",
    "#         crops = crops.astype(int)\n",
    "\n",
    "#         # print(df_val[\"series_id\"][idx])\n",
    "\n",
    "#         if SAVE:\n",
    "#             for i, (x0, y0, x1, y1) in enumerate(crops):\n",
    "#                 crop = imgs[:, y0: y1, x0: x1].copy()\n",
    "#                 assert crop.shape[2] > 1 and crop.shape[1] > 1\n",
    "#                 np.save(SAVE_FOLDER + f'{study_series}_{LEVELS_[i]}.npy', crop)\n",
    "\n",
    "#                 # cc = np.load(SAVE_FOLDER + study_series + \"_\" + LEVELS_[i] + \".npy\")\n",
    "#                 # plt.imshow(cc[len(cc) // 2], cmap=\"gray\")\n",
    "#                 # plt.show()\n",
    "\n",
    "#         if PLOT:\n",
    "#             preds[:, 0] *= imgs.shape[2]\n",
    "#             preds[:, 1] *= imgs.shape[1]\n",
    "\n",
    "#             plt.figure(figsize=(8, 8))\n",
    "#             plt.imshow(imgs[len(imgs) // 2], cmap=\"gray\")\n",
    "#             plt.scatter(preds[:, 0], preds[:, 1], marker=\"x\", label=\"center\")\n",
    "#             # plt.scatter(crops[:, 0], crops[:, 1], marker=\"x\", label=\"top-left\")\n",
    "#             # plt.scatter(crops[:, 2], crops[:, 3], marker=\"x\", label=\"bot-right\")\n",
    "#             plt.title(study_series)\n",
    "#             plt.axis(False)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b7560",
   "metadata": {},
   "source": [
    "## Axial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5742d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sagittal_to_axial import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfee25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_studies = [\n",
    "    # 113758629,\n",
    "    # 13317052, 60612428, 74294498, 142991438, \n",
    "    # 168833126, 189360935, 58813022, 1115952008, 959290081,\n",
    "    2388577668  # bugged\n",
    "]\n",
    "\n",
    "PLOT = False\n",
    "SAVE = True\n",
    "\n",
    "SIZE = 0.1\n",
    "SAVE_FOLDER = f\"../input/crops_ax_{SIZE}/\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "836e446e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'encoder.conv_stem.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:60\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\". \n\tsize mismatch for logits.weight: copying a param with shape torch.Size([4, 5376]) from checkpoint, the shape in current model is torch.Size([4, 768]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:65\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     64\u001b[0m             state_dict_[re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, k)] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m---> 65\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\". \n\tsize mismatch for logits.weight: copying a param with shape torch.Size([4, 5376]) from checkpoint, the shape in current model is torch.Size([4, 768]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:72\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m (\n\u001b[0;32m---> 72\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.classifier.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     73\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.classifier.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.classifier.weight'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:77\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m (\n\u001b[0;32m---> 77\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.head.fc.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.head.fc.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict_, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.head.fc.weight'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:88\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\", \"logits.weight\", \"logits.bias\". ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     20\u001b[0m weights \u001b[38;5;241m=\u001b[39m EXP_FOLDER_AX \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m models_ax\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:90\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     88\u001b[0m             model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.conv_stem.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m             model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.conv_stem.weight'"
     ]
    }
   ],
   "source": [
    "EXP_FOLDER_AX = \"../logs/2024-09-02/33/\"\n",
    "\n",
    "config = Config(json.load(open(EXP_FOLDER_AX + \"config.json\", \"r\")))\n",
    "\n",
    "models_ax = []\n",
    "for fold in range(4):\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        drop_rate=config.drop_rate,\n",
    "        drop_path_rate=config.drop_path_rate,\n",
    "        pooling=config.pooling,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=config.num_classes_aux,\n",
    "        n_channels=config.n_channels,\n",
    "        reduce_stride=config.reduce_stride,\n",
    "        pretrained=False,\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    weights = EXP_FOLDER_AX + f\"{config.name}_{fold}.pt\"\n",
    "    model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "    models_ax.append(model)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a56ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_coords_data()\n",
    "\n",
    "folds = pd.read_csv(config.folds_file)\n",
    "df = df.merge(folds, how=\"left\")\n",
    "df['fold'] = df['fold'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = prepare_data()\n",
    "df_coords = pd.read_csv(DATA_PATH + \"train_label_coordinates.csv\")\n",
    "df_coords['side'] = df_coords['condition'].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "axial_coords = []\n",
    "errors = []\n",
    "pred_sag_coords = []\n",
    "\n",
    "for fold in range(4):\n",
    "    preds_coords = np.load(EXP_FOLDER + f\"pred_val_{fold}.npy\")\n",
    "    df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(range(len(df_val))):\n",
    "        # if idx < 20:\n",
    "        #     continue\n",
    "        \n",
    "        study = df_val[\"study_id\"][idx]\n",
    "        series = df_val[\"series_id\"][idx]\n",
    "\n",
    "        df_s = df_[df_[\"study_id\"] == study]\n",
    "        series_ax = df_s[df_s[\"orient\"] == \"Axial\"].series_id.values[0]\n",
    "\n",
    "        # if not study in ref_studies:\n",
    "        #     continue\n",
    "\n",
    "        # Get axial projection\n",
    "        p = preds_coords[idx].reshape(-1, 2)\n",
    "        # p = fix_predictions(p, df_val[\"target_rel\"][idx])\n",
    "\n",
    "        img = cv2.imread(df_val[\"img_path\"][idx])\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        world_point, assigned_level, closest_z, df_axial = get_axial_coords(\n",
    "            study,\n",
    "            series,\n",
    "            series_ax,\n",
    "            p.copy(),\n",
    "            h,\n",
    "            w,\n",
    "            \"../input/train_images/\",\n",
    "        )\n",
    "\n",
    "        if closest_z.max() == 0:  # Fix\n",
    "            world_point[:, -1] -= (world_point[:, -1].mean() - df_axial.projection.mean())\n",
    "            world_point, assigned_level, closest_z, df_axial = get_axial_coords(\n",
    "                study,\n",
    "                series,\n",
    "                series_ax,\n",
    "                p.copy(),\n",
    "                h,\n",
    "                w,\n",
    "                \"../input/train_images/\",\n",
    "                world_point=world_point\n",
    "            )\n",
    "\n",
    "        # Evaluate\n",
    "        series_ax = df_axial[\"series_id\"].values[0]\n",
    "        df_gt = df_coords[df_coords[\"series_id\"] == series_ax].reset_index(drop=True)\n",
    "        df_gt = df_gt[[\"instance_number\", \"level\", \"x\", \"y\"]].groupby(\"level\").mean().sort_index()\n",
    "        gt = df_gt[\"instance_number\"].values.flatten()\n",
    "        preds = df_axial[\"instance_number\"].values[closest_z]\n",
    "\n",
    "        if len(df_gt) == 5:\n",
    "            mae = np.abs(gt - preds).mean()\n",
    "        else:\n",
    "            mae = 0\n",
    "\n",
    "        # Locate disk\n",
    "        imgs = np.load(f'../input/npy2/{study}_{series_ax}.npy')\n",
    "        imgs_sag = np.load(f'../input/npy2/{study}_{series}.npy')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(imgs[closest_z].astype(np.float32)).cuda()\n",
    "\n",
    "            min_ = x.amin((-1, -2), keepdim=True)\n",
    "            max_ = x.amax((-1, -2), keepdim=True)\n",
    "            x = (x - min_) / (max_ - min_)\n",
    "            x =  F.interpolate(\n",
    "                x.unsqueeze(1).repeat(1, 3, 1, 1),\n",
    "                config.resize,\n",
    "                mode=\"bilinear\",\n",
    "            )\n",
    "\n",
    "            preds_ax = models_ax[fold](x)[0].sigmoid().detach().cpu().numpy().reshape(x.size(0), 2, 2)\n",
    "\n",
    "        preds_ax[:, :, 0] *= imgs.shape[2]\n",
    "        preds_ax[:, :, 1] *= imgs.shape[1]\n",
    "\n",
    "        # GT\n",
    "        df_lvl = df_coords[df_coords[\"series_id\"] == series_ax].reset_index(drop=True)\n",
    "\n",
    "        gts = np.zeros((5, 2, 2))\n",
    "        for i in range(len(LEVELS)):\n",
    "            df_lvl__ = df_lvl[df_lvl[\"level\"] == LEVELS[i]]\n",
    "            for j, s in enumerate([\"Left\", \"Right\"]):\n",
    "                df_lvl_ = df_lvl__[df_lvl__['side'] == s]\n",
    "                if len(df_lvl_):\n",
    "                    gts[i, j] = np.array([df_lvl_['x'].values[0], df_lvl_[\"y\"].values[0]])\n",
    "\n",
    "        preds_ax = np.where(gts > 0, gts, preds_ax)  # FIX\n",
    "        \n",
    "        # Crop\n",
    "        crop_imgs = []\n",
    "        pts, closest_zs_ax = [], []\n",
    "        for i in range(5):\n",
    "            f = closest_z[i]\n",
    "            fs = max(closest_z[i] - 3, 0)\n",
    "            fe = min(closest_z[i] + 3, len(imgs))\n",
    "\n",
    "            xc, yc = preds_ax[i].mean(0).astype(int)\n",
    "\n",
    "            dx, dy = int(imgs.shape[2] * SIZE), int(imgs.shape[1] * SIZE)\n",
    "            x0, x1 = max(xc - dx, 0), min(xc + dx, imgs.shape[2])\n",
    "            y0, y1 = max(yc - dy, 0), min(yc + dy, imgs.shape[1])\n",
    "\n",
    "            d = SIZE // 2\n",
    "            crop = imgs[fs: fe, y0: y1, x0: x1]\n",
    "            crop_imgs.append(crop[len(crop) // 2])\n",
    "\n",
    "            if SAVE:\n",
    "                np.save(SAVE_FOLDER + f\"{study}_{series_ax}_{LEVELS_[i]}.npy\", crop.copy())\n",
    "                np.save(SAVE_FOLDER + f\"{study}_{series}_{LEVELS_[i]}.npy\", crop.copy())\n",
    "\n",
    "            h_ax, w_ax = imgs.shape[1:]\n",
    "\n",
    "            # world_point_ax, assigned_level_ax, closest_z_ax, df_sagittal = get_sagittal_coords(\n",
    "            #     study,\n",
    "            #     series,\n",
    "            #     series_ax,\n",
    "            #     preds_ax[i].copy(),\n",
    "            #     f,\n",
    "            #     1,\n",
    "            #     1,\n",
    "            #     \"../input/train_images/\",\n",
    "            # )\n",
    "            # pts.append(world_point_ax)\n",
    "            # # print(f'Disk {LEVELS[i]} - closest {closest_z_ax}')\n",
    "            # closest_zs_ax.append(closest_z_ax)\n",
    "\n",
    "        # closest_zs_ax = np.array(closest_zs_ax)\n",
    "        # left_frame = int(np.median(closest_zs_ax[:, 0]))\n",
    "        # right_frame = int(np.median(closest_zs_ax[:, 1]))\n",
    "\n",
    "        # df_coords_gt = df_coords[df_coords['study_id'] == study]\n",
    "        # gt_l = int(np.median(df_coords_gt[df_coords_gt['side'] == \"Left\"]['instance_number'].values))\n",
    "        # gt_r = int(np.median(df_coords_gt[df_coords_gt['side'] == \"Right\"]['instance_number'].values))\n",
    "\n",
    "        # error_l = df_sagittal[\"instance_number\"][left_frame] - gt_l\n",
    "        # error_r = df_sagittal[\"instance_number\"][right_frame] - gt_r\n",
    "\n",
    "        # errors.append([error_l, error_r])\n",
    "\n",
    "        # pred_sag_coords.append({\n",
    "        #     \"study_id\": study,\n",
    "        #     \"series_id\": series,\n",
    "        #     \"left\": left_frame,\n",
    "        #     \"right\": right_frame,\n",
    "        # })\n",
    "\n",
    "        # Plot\n",
    "        if PLOT:  #  and (np.abs(error_l) > 5):  # or not (idx % 500):\n",
    "            # display(df_sagittal)\n",
    "            # print(f'Left - pred {left_frame} - truth {gt_l}')\n",
    "            # print(f'Right - pred {right_frame} - truth {gt_r}')\n",
    "\n",
    "            # plot_coords(\n",
    "            #     np.concatenate(pts),\n",
    "            #     assigned_level_ax,\n",
    "            #     closest_z_ax, # np.concatenate(closest_zs_ax),\n",
    "            #     h_ax,\n",
    "            #     w_ax,\n",
    "            #     df_sagittal,\n",
    "            #     title=f\"Study {study} - Series {series}\",\n",
    "            #     orient=\"axial\",\n",
    "            # )\n",
    "\n",
    "            # # display(df_gt)\n",
    "            # plot_coords(\n",
    "            #     world_point,\n",
    "            #     assigned_level,\n",
    "            #     closest_z,\n",
    "            #     h,\n",
    "            #     w,\n",
    "            #     df_axial,\n",
    "            #     title=f\"Study {study} - Series {series_ax}\",\n",
    "            # )\n",
    "\n",
    "        \n",
    "            # plt.figure(figsize=(20, 5))\n",
    "            # plt.subplot(1, 4, 1)\n",
    "            # plt.imshow(imgs_sag[left_frame], cmap=\"gray\")\n",
    "            # plt.title('Pred Left')\n",
    "            # plt.subplot(1, 4, 2)\n",
    "            # plt.imshow(imgs_sag[right_frame], cmap=\"gray\")\n",
    "            # plt.title('Pred Right')\n",
    "            # plt.subplot(1, 4, 3)\n",
    "            # try:\n",
    "            #     plt.imshow(imgs_sag[df_sagittal[\"instance_number\"].values.tolist().index(gt_l)], cmap=\"gray\")\n",
    "            #     plt.title('GT Left')\n",
    "            #     plt.subplot(1, 4, 4)\n",
    "            #     plt.imshow(imgs_sag[df_sagittal[\"instance_number\"].values.tolist().index(gt_r)], cmap=\"gray\")\n",
    "            #     plt.title('GT Right')\n",
    "            # except:\n",
    "            #     pass\n",
    "            # plt.show()\n",
    "            \n",
    "            # Ax coords preds\n",
    "            plt.figure(figsize=(25, 5))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "\n",
    "                img = imgs[closest_z[i]]\n",
    "\n",
    "                plt.scatter(preds_ax[i, :, 0], preds_ax[i, :, 1], label=\"pred\")\n",
    "                plt.scatter(gts[i, :, 0], gts[i, :, 1], label=\"truth\", marker=\"x\")\n",
    "\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                plt.legend()\n",
    "                plt.axis(False)\n",
    "                plt.title(str(df_axial[\"instance_number\"][closest_z[i]]))\n",
    "            plt.show()\n",
    "\n",
    "            # Crops\n",
    "            plt.figure(figsize=(25, 5))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "                plt.imshow(crop_imgs[i], cmap=\"gray\")\n",
    "            plt.show()\n",
    "            \n",
    "            # if PLOT and idx >= 10:\n",
    "            break\n",
    "    # if PLOT:\n",
    "        # if idx >= 100:\n",
    "        #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preds_coords = pd.DataFrame(pred_sag_coords)\n",
    "# df_preds_coords.to_csv('../output/preds_sag_coords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(np.array(errors)[:, 0])\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(np.array(errors)[:, 1])\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(np.abs(np.array(errors)).flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff51fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = []\n",
    "# axial_coords = []\n",
    "\n",
    "# for fold in range(4):\n",
    "#     preds_coords = np.load(EXP_FOLDER + f\"pred_val_{fold}.npy\")\n",
    "#     df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "#     for idx in tqdm(range(len(df_val))):\n",
    "#         study = df_val[\"study_id\"][idx]\n",
    "#         series = df_val[\"series_id\"][idx]\n",
    "\n",
    "#         df_sagittal, _ = read_series_metadata(\n",
    "#             study,\n",
    "#             series,\n",
    "#             \"sagittal\",\n",
    "#             advanced_sorting=False,\n",
    "#             return_imgs=False,\n",
    "#         )\n",
    "#         df_s = df_[df_[\"study_id\"] == study]\n",
    "#         series_ax = df_s[df_s[\"orient\"] == \"Axial\"].series_id.values[0]\n",
    "\n",
    "#         df_axial, _ = read_series_metadata(\n",
    "#             study,\n",
    "#             series_ax,\n",
    "#             \"axial\",\n",
    "#             return_imgs=False,\n",
    "#         )\n",
    "\n",
    "#         imgs_sag = np.load(f\"../input/npy2/{study}_{series}.npy\")\n",
    "#         imgs_ax = np.load(f\"../input/npy2/{study}_{series_ax}.npy\")\n",
    "\n",
    "#         sag_pos = np.array(df_sagittal[\"ImagePositionPatient\"].values.tolist())\n",
    "#         ax_pos = np.array(df_axial[\"ImagePositionPatient\"].values.tolist())\n",
    "\n",
    "#         top_left_hand_corner_sag_t2 = sag_pos[len(sag_pos) // 2]\n",
    "#         spacing = df_sagittal[\"PixelSpacing\"].values[0]\n",
    "#         sag_y_axis_to_pixel_space = [\n",
    "#             top_left_hand_corner_sag_t2[2] - spacing[1] * i for i in range(imgs_sag.shape[1])\n",
    "#         ]\n",
    "\n",
    "#         sag_y_coord_to_axial_slice = {}\n",
    "#         for i, ax_t2_pos in enumerate(ax_pos):\n",
    "#             diffs = np.abs(np.asarray(sag_y_axis_to_pixel_space) - ax_t2_pos[2])\n",
    "#             sag_y_coord = np.argmin(diffs)\n",
    "#             sag_y_coord_to_axial_slice[sag_y_coord] = i\n",
    "\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(imgs_sag[len(imgs_sag) // 2], cmap=\"gray\")\n",
    "#         for k in [*sag_y_coord_to_axial_slice]:\n",
    "#             plt.axhline(y=k, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "#         plt.title(f'{study} {series_ax} - Sagittal to Axial')\n",
    "#         plt.axis(False)\n",
    "\n",
    "#         top_left_hand_corner_ax_t2 = ax_pos[len(ax_pos) // 2]\n",
    "#         spacing = df_axial[\"PixelSpacing\"].values[0]\n",
    "#         ax_x_axis_to_pixel_space = [\n",
    "#             top_left_hand_corner_ax_t2[0] + spacing[0] * i\n",
    "#             for i in range(imgs_ax.shape[2])\n",
    "#         ]\n",
    "\n",
    "#         ax_x_coord_to_sag_slice = {}\n",
    "#         for i, sag_t2_pos in enumerate(sag_pos):\n",
    "#             diffs = np.abs(np.asarray(ax_x_axis_to_pixel_space) - sag_t2_pos[0])\n",
    "#             ax_x_coord = np.argmin(diffs)\n",
    "#             ax_x_coord_to_sag_slice[ax_x_coord] = i\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         img_ax = imgs_ax[len(imgs_ax) // 2]\n",
    "#         plt.imshow(img_ax, cmap=\"gray\")\n",
    "#         for i, k in enumerate([*ax_x_coord_to_sag_slice]):\n",
    "#             alpha = 0.5 if i == len(ax_x_coord_to_sag_slice) // 2 else 0.3\n",
    "#             plt.axvline(x=k, color=\"red\", linestyle=\"--\", alpha=alpha)\n",
    "#         plt.title(f'{study} {series_ax} - Axial to Sagittal')\n",
    "#         plt.axis(False)\n",
    "#         plt.show()\n",
    "\n",
    "#         if idx > 10:\n",
    "#             break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ca4c4",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6677.910014,
   "end_time": "2023-08-12T17:53:47.148086",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T16:02:29.238072",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
