{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fefbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5d4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_rsna_lumbar_spine/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e6ad4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.317669,
     "end_time": "2023-08-12T16:03:02.827155",
     "exception": false,
     "start_time": "2023-08-12T16:03:00.509486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d620e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "\n",
    "from data.dataset import *\n",
    "from data.transforms import *\n",
    "from data.preparation import *\n",
    "from data.processing import read_series_metadata\n",
    "from data.sagittal_to_axial import get_axial_coords\n",
    "\n",
    "from model_zoo.models import define_model\n",
    "from util.torch import load_model_weights\n",
    "from util.plots import *\n",
    "\n",
    "from inference.lvl1 import Config, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0980a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_crop(DATA_PATH, crop_folder=\"../input/crops_0.15_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357cb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prepare_data_nfn(DATA_PATH, crop_folder=\"../input/crops_0.15_2/\")\n",
    "# df = df[df['side'] == 'Left']\n",
    "# df_preds_coords = pd.read_csv('../output/seg_sag_coords.csv')\n",
    "# df = df.merge(df_preds_coords, how=\"left\")\n",
    "# delta = df.apply(lambda x: x.left - x.coords[0] if x.side == \"Left\" else x.right - x.coords[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca0ec4",
   "metadata": {},
   "source": [
    "## Sagittal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eccccf",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5acf7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2024-08-28/13/\"  # coatnet_rmlp_2_rw_384 50 ep new folds\n",
    "EXP_FOLDER = \"../logs/2024-08-28/24/\"  # coatnet_rmlp_2_rw_384 aug 50 ep new folds\n",
    "EXP_FOLDER = \"../logs/2024-08-29/0/\"  # coatnet_rmlp_2_rw_384 aug 50 ep new folds\n",
    "\n",
    "EXP_FOLDER = \"../output/2024-08-29_0/\"  # coatnet_rmlp_2_rw_384 aug 50 ep new folds\n",
    "\n",
    "FOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eea1d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = pd.read_csv(DATA_PATH + 'train_label_coordinates_v0.csv')\n",
    "# coords = coords[coords['condition'] == \"Spinal Canal Stenosis\"].sort_values([\"study_id\", \"series_id\", 'level'])\n",
    "# coords = coords.groupby([\"study_id\", \"series_id\"]).agg(list).reset_index()\n",
    "# coords = coords[coords['level'].apply(len) != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcadd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "# df_y[df_y.isna().values[:, :15].sum(1) > 0]\n",
    "# os.listdir('../input/train_images/2492114990/')\n",
    "# df_gt = prepare_data_scs(explode=False)\n",
    "# df_gt = df_gt[df_gt[\"level\"].apply(lambda x: 0 if isinstance(x, float) else len(x)) != 5]\n",
    "# df_gt.head(1)\n",
    "\n",
    "# for i, row in df_gt.iterrows():\n",
    "#     try:\n",
    "#         missing = [j for j, r in enumerate(LEVELS)  if r not in row.level]\n",
    "#     except:\n",
    "#         missing = range(5)\n",
    "\n",
    "#     for i in missing:\n",
    "#         print(\"Missing disk\", LEVELS[i], \" - target\", row[CLASSES_SCS[i]])\n",
    "#     print()\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "617fcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "\n",
    "df = prepare_coords_data(config.coords_folder, use_ext=config.use_ext)\n",
    "\n",
    "folds = pd.read_csv(config.folds_file)\n",
    "df = df.merge(folds, how=\"left\")\n",
    "df['fold'] = df['fold'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "903f63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models_sag = []\n",
    "# for fold in range(4):\n",
    "#     model = define_model(\n",
    "#         config.name,\n",
    "#         drop_rate=config.drop_rate,\n",
    "#         drop_path_rate=config.drop_path_rate,\n",
    "#         pooling=config.pooling,\n",
    "#         num_classes=config.num_classes,\n",
    "#         num_classes_aux=config.num_classes_aux,\n",
    "#         n_channels=config.n_channels,\n",
    "#         reduce_stride=config.reduce_stride,\n",
    "#         pretrained=False,\n",
    "#     )\n",
    "#     model = model.cuda().eval()\n",
    "\n",
    "#     weights = EXP_FOLDER + f\"{config.name}_{fold}.pt\"\n",
    "#     try:\n",
    "#         model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "#     except FileNotFoundError:\n",
    "#         continue\n",
    "#     # models_sag.append(model)\n",
    "\n",
    "#     df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "#     dataset = CoordsDataset(df_val, transforms=get_transfos(augment=False, resize=config.resize, use_keypoints=True))\n",
    "\n",
    "#     preds, _ = predict(model, dataset, config.loss_config, batch_size=32, use_fp16=True)\n",
    "\n",
    "#     np.save(EXP_FOLDER + f\"pred_inf_{fold}.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce921ddc",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76563826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = pd.read_csv('../input/train_label_coordinates.csv')\n",
    "\n",
    "# cc = cc.merge(\n",
    "#     cc[[\"series_id\", \"level\"]].groupby(\"series_id\").count().reset_index(),\n",
    "#     how=\"left\",\n",
    "#     on=\"series_id\",\n",
    "#     suffixes=(\"\", \"_count\"),\n",
    "# )\n",
    "\n",
    "# dfg = (\n",
    "#     cc[(cc.condition.isin([\"Right Neural Foraminal Narrowing\", \"Left Neural Foraminal Narrowing\"])) & ~(cc.level_count.isin([10]))]\n",
    "#     .groupby([\"study_id\", \"series_id\"])\n",
    "#     .agg(list)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# dfg.shape\n",
    "\n",
    "# # dfg = (\n",
    "# #     cc[(cc.condition == \"Spinal Canal Stenosis\") & (cc.level_count != 5)]\n",
    "# #     .sort_values(\"level\")\n",
    "# #     .groupby([\"study_id\", \"series_id\"])\n",
    "# #     .agg(list)\n",
    "# #     .reset_index()\n",
    "# # )\n",
    "# # dfg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2354a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO_FIX = [\n",
    "#     # 1468566581,\n",
    "#     # 2185202709,\n",
    "#     # 716946645,\n",
    "#     # 3123114360,\n",
    "#     # 2143604834,\n",
    "#     # 1540250849,\n",
    "#     # 2943022937,\n",
    "#     # 425681838,\n",
    "#     # 1638921810,\n",
    "#     # 4089223112,\n",
    "#     # 648725109,\n",
    "#     # 2177693773,\n",
    "#     # 2929396535,\n",
    "#     # 4245678886,\n",
    "#     # 2538968579,\n",
    "# ]\n",
    "\n",
    "# def shift_disk_up():\n",
    "#     df = pd.read_csv('../input/train_label_coordinates.csv')\n",
    "\n",
    "#     for series in TO_FIX:\n",
    "#         df_fixes = df.loc[df['series_id'] == series].sort_values('level').reset_index(drop=True)\n",
    "#         df = df[df['series_id'] != series].reset_index(drop=True)\n",
    "\n",
    "#         for _, df_fix  in df_fixes.groupby('condition'):\n",
    "#             df_fix['level'] = df_fix['level'].map({'L1/L2': \"L2/L3\", 'L2/L3': \"L3/L4\", 'L3/L4': \"L4/L5\", 'L4/L5': \"L5/S1\"})\n",
    "#             df_fix = df_fix.dropna(axis=0)\n",
    "#             df = pd.concat([df, df_fix], ignore_index=True)\n",
    "\n",
    "#     df.to_csv('../input/train_label_coordinates_shift.csv')\n",
    "\n",
    "# # shift_disk_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29e1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2db78c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = prepare_data()\n",
    "df_sev = prepare_data_crop(DATA_PATH)\n",
    "df_spinenet = pd.read_csv('../output/spinenet_kps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b98402b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDIES = df_gt[df_gt['series_id'].isin(TO_FIX)].study_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5c879d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_predictions(p, y, verbose=0):\n",
    "#     d = np.abs(p - y) * 100\n",
    "#     d = d[y.sum(-1) > 0].mean()\n",
    "\n",
    "#     if y.min() > 0 and d > 1:\n",
    "#         return y\n",
    "\n",
    "#     delta = (np.random.random() - 0.5) / 100\n",
    "    \n",
    "#     # Shift down\n",
    "#     p_down = p[1:]\n",
    "#     y_down = y[:-1]\n",
    "#     d_down = np.abs(p_down - y_down) * 100\n",
    "#     d_down = d_down[y_down.sum(-1) > 0].mean()\n",
    "\n",
    "#     # Shift up\n",
    "#     p_up = p[:-1]\n",
    "#     y_up = y[1:]\n",
    "#     d_up = np.abs(p_up - y_up) * 100\n",
    "#     d_up = d_up[y_up.sum(-1) > 0].mean()\n",
    "\n",
    "#     if d_up < d:  # shift up\n",
    "#         if verbose:\n",
    "#             print('Shift up')\n",
    "#         if y[0].sum() > 0:\n",
    "#             fix = y[0] + delta\n",
    "#         else:\n",
    "#             fix = p[0] + np.array([0.03, -0.09])\n",
    "#         return np.vstack([fix, p_up])\n",
    "\n",
    "#     elif d_down < d:  # shift down\n",
    "#         if verbose:\n",
    "#             print('Shift down')\n",
    "#         if y[-1].sum() > 0:\n",
    "#             fix = y[-1] + delta\n",
    "#         else:\n",
    "#             fix = p[-1] + np.array([0.04, 0.09])\n",
    "#         return np.vstack([p_down, fix])\n",
    "\n",
    "#     else:\n",
    "#         return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bbea599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [00:10<00:00, 92.73it/s] \n"
     ]
    }
   ],
   "source": [
    "ds = []\n",
    "df_ = prepare_data()\n",
    "\n",
    "for fold in range(4):\n",
    "    preds = np.load(EXP_FOLDER + f\"pred_inf_{fold}.npy\")\n",
    "    df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    dataset = CoordsDataset(df_val, transforms=get_transfos(augment=False, use_keypoints=True))\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        study = df_val['study_id'][idx]\n",
    "        series = df_val['series_id'][idx]\n",
    "\n",
    "        # if df_val['series_description'][idx] != \"Sagittal T2/STIR\":\n",
    "        #     continue\n",
    "\n",
    "        # if series in TO_FIX:\n",
    "        #     continue\n",
    "        # if not study in STUDIES:\n",
    "        #     continue\n",
    "\n",
    "        img, y, _ = dataset[idx]\n",
    "        labels = np.vstack(df_sev[df_sev['series_id'] == series].sort_values('level')['target'].values)\n",
    "\n",
    "        gt = df_gt[df_gt['series_id'] == series]\n",
    "        imgs = np.load(f'../input/npy2/{study}_{series}.npy')\n",
    "\n",
    "        # print(gt['coords'].values[0][:, 0])\n",
    "        # frame = int(np.round(gt['coords'].values[0][:, 0].mean()))\n",
    "        frame = len(imgs) // 2\n",
    "        # frame = len(imgs) // 4\n",
    "        # frame = int(gt['coords'].values[0][-1, 0])\n",
    "\n",
    "        img = imgs[frame]\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "        try:\n",
    "            spinenet_coords = df_spinenet[df_spinenet['series_id'] == series]\n",
    "            spinenet_coords = spinenet_coords.values[len(spinenet_coords)  // 2, -10:].reshape(2, 5).T\n",
    "            spinenet_coords[:, 0] /= img.shape[1]\n",
    "            spinenet_coords[:, 0] /= img.shape[0]\n",
    "            # p = spinenet_coords.copy()\n",
    "            # p[:, 0] /= img.shape[1]\n",
    "            # p[:, 1] /= img.shape[0]\n",
    "        except:\n",
    "            spinenet_coords = None\n",
    "\n",
    "        p_ = preds[idx].reshape(-1, 2)\n",
    "        p = preds[idx].reshape(-1, 2)\n",
    "\n",
    "        # p = fix_predictions(p, y.numpy())\n",
    "        p_ = p.copy()\n",
    "\n",
    "        d = np.abs(p - y.numpy()) * 100\n",
    "        d = d[y.sum(-1) > 0].mean()\n",
    "        ds.append(d)\n",
    "\n",
    "        if PLOT:\n",
    "            if d > 3:\n",
    "                y = y[y.sum(-1) > 0]\n",
    "                # if len(y) == 5:\n",
    "                #     continue\n",
    "\n",
    "                print(study, series)\n",
    "                print('SCS / L-NFN / R-NFN / L-SS / R-SS')\n",
    "                print(labels)\n",
    "\n",
    "                # cv2.imwrite(f'../output/fix/{study}_{series}.png', (img * 255).astype(np.uint8))\n",
    "\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                plt.scatter(y[:, 0] * img.shape[1], y[:, 1] * img.shape[0], marker=\"x\", label=\"truth\")\n",
    "                plt.scatter(p_[:, 0] * img.shape[1], p_[:, 1] * img.shape[0], marker=\"x\", label=\"pred\")\n",
    "                if spinenet_coords is not None:\n",
    "                    plt.scatter(spinenet_coords[:, 0], spinenet_coords[:, 1], marker=\"x\", label=\"spinenet\")\n",
    "                plt.title(f'Dist = {d:.2f} - study {study} - series {series}')\n",
    "                plt.axis(False)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                print(p_[:, 0] * img.shape[1], p_[:, 1] * img.shape[0])\n",
    "\n",
    "            # if idx > 50:\n",
    "                # break\n",
    "    # if PLOT:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c1d6ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with error > 5%: 6\n",
      "Images with error > 4%: 9\n",
      "Images with error > 3%: 21\n"
     ]
    }
   ],
   "source": [
    "print('Images with error > 5%:', (np.array(ds) > 5).sum())\n",
    "print('Images with error > 4%:', (np.array(ds) > 4).sum())\n",
    "print('Images with error > 3%:', (np.array(ds) > 3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dc35bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(ds)\n",
    "# plt.axvline(np.mean(ds), c=\"salmon\")\n",
    "# plt.text(np.mean(ds), 100, f\"   mean={np.mean(ds):.3f}\", color=\"salmon\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64045e0",
   "metadata": {},
   "source": [
    "### Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46f88900",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 0.15\n",
    "\n",
    "SAVE = True\n",
    "PLOT = False\n",
    "\n",
    "SAVE_FOLDER = f\"../input/coords_crops_{DELTA}_2/\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "020f0bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [00:36<00:00, 27.27it/s]\n",
      "100%|██████████| 989/989 [00:36<00:00, 27.27it/s]\n",
      "100%|██████████| 988/988 [00:36<00:00, 26.72it/s]\n",
      "100%|██████████| 984/984 [00:36<00:00, 26.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4):\n",
    "    pred_val = np.load(EXP_FOLDER + f\"pred_inf_{fold}.npy\")\n",
    "    df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(range(len(df_val))):\n",
    "        study_series = df_val[\"img_path\"][idx].split('/')[-1][:-4]\n",
    "        imgs_path = DATA_PATH + \"npy2/\" + study_series + \".npy\"      ###### NPY2 ??\n",
    "        imgs = np.load(imgs_path)\n",
    "        img = imgs[0]\n",
    "\n",
    "        # try:\n",
    "        #     series = df_val[\"series_id\"][idx]\n",
    "        #     spinenet_coords = df_spinenet[df_spinenet['series_id'] == series]\n",
    "        #     spinenet_coords = spinenet_coords.values[len(spinenet_coords)  // 2, -10:].reshape(2, 5).T\n",
    "        #     spinenet_coords[:, 0] /= img.shape[1]\n",
    "        #     spinenet_coords[:, 1] /= img.shape[0]\n",
    "        #     preds = spinenet_coords.copy()\n",
    "        # except IndexError:\n",
    "        #     print('No Spinenet coords found')\n",
    "        preds = pred_val[idx].reshape(-1, 2)\n",
    "        # preds = fix_predictions(preds, df_val[\"target_rel\"][idx], verbose=1)\n",
    "\n",
    "        assert preds.min() >= 0\n",
    "        assert preds.max() <= 1\n",
    "\n",
    "        crops = np.concatenate([preds, preds], -1)\n",
    "        crops[:, [0, 1]] -= DELTA\n",
    "        crops[:, [2, 3]] += DELTA\n",
    "        crops = crops.clip(0, 1)\n",
    "        crops[:, [0, 2]] *= imgs.shape[2]\n",
    "        crops[:, [1, 3]] *= imgs.shape[1]\n",
    "        crops = crops.astype(int)\n",
    "\n",
    "        # print(df_val[\"series_id\"][idx])\n",
    "\n",
    "        if SAVE:\n",
    "            for i, (x0, y0, x1, y1) in enumerate(crops):\n",
    "                crop = imgs[:, y0: y1, x0: x1].copy()\n",
    "                assert crop.shape[2] > 1 and crop.shape[1] > 1\n",
    "                np.save(SAVE_FOLDER + f'{study_series}_{LEVELS_[i]}.npy', crop)\n",
    "\n",
    "                # cc = np.load(SAVE_FOLDER + study_series + \"_\" + LEVELS_[i] + \".npy\")\n",
    "                # plt.imshow(cc[len(cc) // 2], cmap=\"gray\")\n",
    "                # plt.show()\n",
    "\n",
    "        if PLOT:\n",
    "            preds[:, 0] *= imgs.shape[2]\n",
    "            preds[:, 1] *= imgs.shape[1]\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(imgs[len(imgs) // 2], cmap=\"gray\")\n",
    "            plt.scatter(preds[:, 0], preds[:, 1], marker=\"x\", label=\"center\")\n",
    "            # plt.scatter(crops[:, 0], crops[:, 1], marker=\"x\", label=\"top-left\")\n",
    "            # plt.scatter(crops[:, 2], crops[:, 3], marker=\"x\", label=\"bot-right\")\n",
    "            plt.title(study_series)\n",
    "            plt.axis(False)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b7560",
   "metadata": {},
   "source": [
    "## Axial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5742d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sagittal_to_axial import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfee25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_studies = [\n",
    "    # 113758629,\n",
    "    # 13317052, 60612428, 74294498, 142991438, \n",
    "    # 168833126, 189360935, 58813022, 1115952008, 959290081,\n",
    "    2388577668  # bugged\n",
    "]\n",
    "\n",
    "PLOT = False\n",
    "SAVE = True\n",
    "\n",
    "SIZE = 0.1\n",
    "SAVE_FOLDER = f\"../input/crops_ax_{SIZE}/\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "836e446e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'encoder.conv_stem.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:60\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\". \n\tsize mismatch for logits.weight: copying a param with shape torch.Size([4, 5376]) from checkpoint, the shape in current model is torch.Size([4, 768]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:65\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     64\u001b[0m             state_dict_[re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, k)] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m---> 65\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\". \n\tsize mismatch for logits.weight: copying a param with shape torch.Size([4, 5376]) from checkpoint, the shape in current model is torch.Size([4, 768]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:72\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m (\n\u001b[0;32m---> 72\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.classifier.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     73\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.classifier.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.classifier.weight'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:77\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m (\n\u001b[0;32m---> 77\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.head.fc.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m         state_dict_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.head.fc.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict_, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.head.fc.weight'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:88\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClsModel:\n\tMissing key(s) in state_dict: \"dense.0.weight\", \"dense.0.bias\", \"logits.weight\", \"logits.bias\". ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     20\u001b[0m weights \u001b[38;5;241m=\u001b[39m EXP_FOLDER_AX \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m models_ax\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[0;32m~/work/kaggle_rsna_lumbar_spine/src/util/torch.py:90\u001b[0m, in \u001b[0;36mload_model_weights\u001b[0;34m(model, filename, verbose, cp_folder, strict)\u001b[0m\n\u001b[1;32m     88\u001b[0m             model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.conv_stem.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m             model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder.conv_stem.weight'"
     ]
    }
   ],
   "source": [
    "EXP_FOLDER_AX = \"../logs/2024-09-02/33/\"\n",
    "\n",
    "config = Config(json.load(open(EXP_FOLDER_AX + \"config.json\", \"r\")))\n",
    "\n",
    "models_ax = []\n",
    "for fold in range(4):\n",
    "    model = define_model(\n",
    "        config.name,\n",
    "        drop_rate=config.drop_rate,\n",
    "        drop_path_rate=config.drop_path_rate,\n",
    "        pooling=config.pooling,\n",
    "        num_classes=config.num_classes,\n",
    "        num_classes_aux=config.num_classes_aux,\n",
    "        n_channels=config.n_channels,\n",
    "        reduce_stride=config.reduce_stride,\n",
    "        pretrained=False,\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    weights = EXP_FOLDER_AX + f\"{config.name}_{fold}.pt\"\n",
    "    model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "    models_ax.append(model)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a56ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_coords_data()\n",
    "\n",
    "folds = pd.read_csv(config.folds_file)\n",
    "df = df.merge(folds, how=\"left\")\n",
    "df['fold'] = df['fold'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = prepare_data()\n",
    "df_coords = pd.read_csv(DATA_PATH + \"train_label_coordinates.csv\")\n",
    "df_coords['side'] = df_coords['condition'].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "axial_coords = []\n",
    "errors = []\n",
    "pred_sag_coords = []\n",
    "\n",
    "for fold in range(4):\n",
    "    preds_coords = np.load(EXP_FOLDER + f\"pred_val_{fold}.npy\")\n",
    "    df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(range(len(df_val))):\n",
    "        # if idx < 20:\n",
    "        #     continue\n",
    "        \n",
    "        study = df_val[\"study_id\"][idx]\n",
    "        series = df_val[\"series_id\"][idx]\n",
    "\n",
    "        df_s = df_[df_[\"study_id\"] == study]\n",
    "        series_ax = df_s[df_s[\"orient\"] == \"Axial\"].series_id.values[0]\n",
    "\n",
    "        # if not study in ref_studies:\n",
    "        #     continue\n",
    "\n",
    "        # Get axial projection\n",
    "        p = preds_coords[idx].reshape(-1, 2)\n",
    "        # p = fix_predictions(p, df_val[\"target_rel\"][idx])\n",
    "\n",
    "        img = cv2.imread(df_val[\"img_path\"][idx])\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        world_point, assigned_level, closest_z, df_axial = get_axial_coords(\n",
    "            study,\n",
    "            series,\n",
    "            series_ax,\n",
    "            p.copy(),\n",
    "            h,\n",
    "            w,\n",
    "            \"../input/train_images/\",\n",
    "        )\n",
    "\n",
    "        if closest_z.max() == 0:  # Fix\n",
    "            world_point[:, -1] -= (world_point[:, -1].mean() - df_axial.projection.mean())\n",
    "            world_point, assigned_level, closest_z, df_axial = get_axial_coords(\n",
    "                study,\n",
    "                series,\n",
    "                series_ax,\n",
    "                p.copy(),\n",
    "                h,\n",
    "                w,\n",
    "                \"../input/train_images/\",\n",
    "                world_point=world_point\n",
    "            )\n",
    "\n",
    "        # Evaluate\n",
    "        series_ax = df_axial[\"series_id\"].values[0]\n",
    "        df_gt = df_coords[df_coords[\"series_id\"] == series_ax].reset_index(drop=True)\n",
    "        df_gt = df_gt[[\"instance_number\", \"level\", \"x\", \"y\"]].groupby(\"level\").mean().sort_index()\n",
    "        gt = df_gt[\"instance_number\"].values.flatten()\n",
    "        preds = df_axial[\"instance_number\"].values[closest_z]\n",
    "\n",
    "        if len(df_gt) == 5:\n",
    "            mae = np.abs(gt - preds).mean()\n",
    "        else:\n",
    "            mae = 0\n",
    "\n",
    "        # Locate disk\n",
    "        imgs = np.load(f'../input/npy2/{study}_{series_ax}.npy')\n",
    "        imgs_sag = np.load(f'../input/npy2/{study}_{series}.npy')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(imgs[closest_z].astype(np.float32)).cuda()\n",
    "\n",
    "            min_ = x.amin((-1, -2), keepdim=True)\n",
    "            max_ = x.amax((-1, -2), keepdim=True)\n",
    "            x = (x - min_) / (max_ - min_)\n",
    "            x =  F.interpolate(\n",
    "                x.unsqueeze(1).repeat(1, 3, 1, 1),\n",
    "                config.resize,\n",
    "                mode=\"bilinear\",\n",
    "            )\n",
    "\n",
    "            preds_ax = models_ax[fold](x)[0].sigmoid().detach().cpu().numpy().reshape(x.size(0), 2, 2)\n",
    "\n",
    "        preds_ax[:, :, 0] *= imgs.shape[2]\n",
    "        preds_ax[:, :, 1] *= imgs.shape[1]\n",
    "\n",
    "        # GT\n",
    "        df_lvl = df_coords[df_coords[\"series_id\"] == series_ax].reset_index(drop=True)\n",
    "\n",
    "        gts = np.zeros((5, 2, 2))\n",
    "        for i in range(len(LEVELS)):\n",
    "            df_lvl__ = df_lvl[df_lvl[\"level\"] == LEVELS[i]]\n",
    "            for j, s in enumerate([\"Left\", \"Right\"]):\n",
    "                df_lvl_ = df_lvl__[df_lvl__['side'] == s]\n",
    "                if len(df_lvl_):\n",
    "                    gts[i, j] = np.array([df_lvl_['x'].values[0], df_lvl_[\"y\"].values[0]])\n",
    "\n",
    "        preds_ax = np.where(gts > 0, gts, preds_ax)  # FIX\n",
    "        \n",
    "        # Crop\n",
    "        crop_imgs = []\n",
    "        pts, closest_zs_ax = [], []\n",
    "        for i in range(5):\n",
    "            f = closest_z[i]\n",
    "            fs = max(closest_z[i] - 3, 0)\n",
    "            fe = min(closest_z[i] + 3, len(imgs))\n",
    "\n",
    "            xc, yc = preds_ax[i].mean(0).astype(int)\n",
    "\n",
    "            dx, dy = int(imgs.shape[2] * SIZE), int(imgs.shape[1] * SIZE)\n",
    "            x0, x1 = max(xc - dx, 0), min(xc + dx, imgs.shape[2])\n",
    "            y0, y1 = max(yc - dy, 0), min(yc + dy, imgs.shape[1])\n",
    "\n",
    "            d = SIZE // 2\n",
    "            crop = imgs[fs: fe, y0: y1, x0: x1]\n",
    "            crop_imgs.append(crop[len(crop) // 2])\n",
    "\n",
    "            if SAVE:\n",
    "                np.save(SAVE_FOLDER + f\"{study}_{series_ax}_{LEVELS_[i]}.npy\", crop.copy())\n",
    "                np.save(SAVE_FOLDER + f\"{study}_{series}_{LEVELS_[i]}.npy\", crop.copy())\n",
    "\n",
    "            h_ax, w_ax = imgs.shape[1:]\n",
    "\n",
    "            # world_point_ax, assigned_level_ax, closest_z_ax, df_sagittal = get_sagittal_coords(\n",
    "            #     study,\n",
    "            #     series,\n",
    "            #     series_ax,\n",
    "            #     preds_ax[i].copy(),\n",
    "            #     f,\n",
    "            #     1,\n",
    "            #     1,\n",
    "            #     \"../input/train_images/\",\n",
    "            # )\n",
    "            # pts.append(world_point_ax)\n",
    "            # # print(f'Disk {LEVELS[i]} - closest {closest_z_ax}')\n",
    "            # closest_zs_ax.append(closest_z_ax)\n",
    "\n",
    "        # closest_zs_ax = np.array(closest_zs_ax)\n",
    "        # left_frame = int(np.median(closest_zs_ax[:, 0]))\n",
    "        # right_frame = int(np.median(closest_zs_ax[:, 1]))\n",
    "\n",
    "        # df_coords_gt = df_coords[df_coords['study_id'] == study]\n",
    "        # gt_l = int(np.median(df_coords_gt[df_coords_gt['side'] == \"Left\"]['instance_number'].values))\n",
    "        # gt_r = int(np.median(df_coords_gt[df_coords_gt['side'] == \"Right\"]['instance_number'].values))\n",
    "\n",
    "        # error_l = df_sagittal[\"instance_number\"][left_frame] - gt_l\n",
    "        # error_r = df_sagittal[\"instance_number\"][right_frame] - gt_r\n",
    "\n",
    "        # errors.append([error_l, error_r])\n",
    "\n",
    "        # pred_sag_coords.append({\n",
    "        #     \"study_id\": study,\n",
    "        #     \"series_id\": series,\n",
    "        #     \"left\": left_frame,\n",
    "        #     \"right\": right_frame,\n",
    "        # })\n",
    "\n",
    "        # Plot\n",
    "        if PLOT:  #  and (np.abs(error_l) > 5):  # or not (idx % 500):\n",
    "            # display(df_sagittal)\n",
    "            # print(f'Left - pred {left_frame} - truth {gt_l}')\n",
    "            # print(f'Right - pred {right_frame} - truth {gt_r}')\n",
    "\n",
    "            # plot_coords(\n",
    "            #     np.concatenate(pts),\n",
    "            #     assigned_level_ax,\n",
    "            #     closest_z_ax, # np.concatenate(closest_zs_ax),\n",
    "            #     h_ax,\n",
    "            #     w_ax,\n",
    "            #     df_sagittal,\n",
    "            #     title=f\"Study {study} - Series {series}\",\n",
    "            #     orient=\"axial\",\n",
    "            # )\n",
    "\n",
    "            # # display(df_gt)\n",
    "            # plot_coords(\n",
    "            #     world_point,\n",
    "            #     assigned_level,\n",
    "            #     closest_z,\n",
    "            #     h,\n",
    "            #     w,\n",
    "            #     df_axial,\n",
    "            #     title=f\"Study {study} - Series {series_ax}\",\n",
    "            # )\n",
    "\n",
    "        \n",
    "            # plt.figure(figsize=(20, 5))\n",
    "            # plt.subplot(1, 4, 1)\n",
    "            # plt.imshow(imgs_sag[left_frame], cmap=\"gray\")\n",
    "            # plt.title('Pred Left')\n",
    "            # plt.subplot(1, 4, 2)\n",
    "            # plt.imshow(imgs_sag[right_frame], cmap=\"gray\")\n",
    "            # plt.title('Pred Right')\n",
    "            # plt.subplot(1, 4, 3)\n",
    "            # try:\n",
    "            #     plt.imshow(imgs_sag[df_sagittal[\"instance_number\"].values.tolist().index(gt_l)], cmap=\"gray\")\n",
    "            #     plt.title('GT Left')\n",
    "            #     plt.subplot(1, 4, 4)\n",
    "            #     plt.imshow(imgs_sag[df_sagittal[\"instance_number\"].values.tolist().index(gt_r)], cmap=\"gray\")\n",
    "            #     plt.title('GT Right')\n",
    "            # except:\n",
    "            #     pass\n",
    "            # plt.show()\n",
    "            \n",
    "            # Ax coords preds\n",
    "            plt.figure(figsize=(25, 5))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "\n",
    "                img = imgs[closest_z[i]]\n",
    "\n",
    "                plt.scatter(preds_ax[i, :, 0], preds_ax[i, :, 1], label=\"pred\")\n",
    "                plt.scatter(gts[i, :, 0], gts[i, :, 1], label=\"truth\", marker=\"x\")\n",
    "\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                plt.legend()\n",
    "                plt.axis(False)\n",
    "                plt.title(str(df_axial[\"instance_number\"][closest_z[i]]))\n",
    "            plt.show()\n",
    "\n",
    "            # Crops\n",
    "            plt.figure(figsize=(25, 5))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i + 1)\n",
    "                plt.imshow(crop_imgs[i], cmap=\"gray\")\n",
    "            plt.show()\n",
    "            \n",
    "            # if PLOT and idx >= 10:\n",
    "            break\n",
    "    # if PLOT:\n",
    "        # if idx >= 100:\n",
    "        #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preds_coords = pd.DataFrame(pred_sag_coords)\n",
    "# df_preds_coords.to_csv('../output/preds_sag_coords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(np.array(errors)[:, 0])\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(np.array(errors)[:, 1])\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(np.abs(np.array(errors)).flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff51fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = []\n",
    "# axial_coords = []\n",
    "\n",
    "# for fold in range(4):\n",
    "#     preds_coords = np.load(EXP_FOLDER + f\"pred_val_{fold}.npy\")\n",
    "#     df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "#     for idx in tqdm(range(len(df_val))):\n",
    "#         study = df_val[\"study_id\"][idx]\n",
    "#         series = df_val[\"series_id\"][idx]\n",
    "\n",
    "#         df_sagittal, _ = read_series_metadata(\n",
    "#             study,\n",
    "#             series,\n",
    "#             \"sagittal\",\n",
    "#             advanced_sorting=False,\n",
    "#             return_imgs=False,\n",
    "#         )\n",
    "#         df_s = df_[df_[\"study_id\"] == study]\n",
    "#         series_ax = df_s[df_s[\"orient\"] == \"Axial\"].series_id.values[0]\n",
    "\n",
    "#         df_axial, _ = read_series_metadata(\n",
    "#             study,\n",
    "#             series_ax,\n",
    "#             \"axial\",\n",
    "#             return_imgs=False,\n",
    "#         )\n",
    "\n",
    "#         imgs_sag = np.load(f\"../input/npy2/{study}_{series}.npy\")\n",
    "#         imgs_ax = np.load(f\"../input/npy2/{study}_{series_ax}.npy\")\n",
    "\n",
    "#         sag_pos = np.array(df_sagittal[\"ImagePositionPatient\"].values.tolist())\n",
    "#         ax_pos = np.array(df_axial[\"ImagePositionPatient\"].values.tolist())\n",
    "\n",
    "#         top_left_hand_corner_sag_t2 = sag_pos[len(sag_pos) // 2]\n",
    "#         spacing = df_sagittal[\"PixelSpacing\"].values[0]\n",
    "#         sag_y_axis_to_pixel_space = [\n",
    "#             top_left_hand_corner_sag_t2[2] - spacing[1] * i for i in range(imgs_sag.shape[1])\n",
    "#         ]\n",
    "\n",
    "#         sag_y_coord_to_axial_slice = {}\n",
    "#         for i, ax_t2_pos in enumerate(ax_pos):\n",
    "#             diffs = np.abs(np.asarray(sag_y_axis_to_pixel_space) - ax_t2_pos[2])\n",
    "#             sag_y_coord = np.argmin(diffs)\n",
    "#             sag_y_coord_to_axial_slice[sag_y_coord] = i\n",
    "\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(imgs_sag[len(imgs_sag) // 2], cmap=\"gray\")\n",
    "#         for k in [*sag_y_coord_to_axial_slice]:\n",
    "#             plt.axhline(y=k, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "#         plt.title(f'{study} {series_ax} - Sagittal to Axial')\n",
    "#         plt.axis(False)\n",
    "\n",
    "#         top_left_hand_corner_ax_t2 = ax_pos[len(ax_pos) // 2]\n",
    "#         spacing = df_axial[\"PixelSpacing\"].values[0]\n",
    "#         ax_x_axis_to_pixel_space = [\n",
    "#             top_left_hand_corner_ax_t2[0] + spacing[0] * i\n",
    "#             for i in range(imgs_ax.shape[2])\n",
    "#         ]\n",
    "\n",
    "#         ax_x_coord_to_sag_slice = {}\n",
    "#         for i, sag_t2_pos in enumerate(sag_pos):\n",
    "#             diffs = np.abs(np.asarray(ax_x_axis_to_pixel_space) - sag_t2_pos[0])\n",
    "#             ax_x_coord = np.argmin(diffs)\n",
    "#             ax_x_coord_to_sag_slice[ax_x_coord] = i\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         img_ax = imgs_ax[len(imgs_ax) // 2]\n",
    "#         plt.imshow(img_ax, cmap=\"gray\")\n",
    "#         for i, k in enumerate([*ax_x_coord_to_sag_slice]):\n",
    "#             alpha = 0.5 if i == len(ax_x_coord_to_sag_slice) // 2 else 0.3\n",
    "#             plt.axvline(x=k, color=\"red\", linestyle=\"--\", alpha=alpha)\n",
    "#         plt.title(f'{study} {series_ax} - Axial to Sagittal')\n",
    "#         plt.axis(False)\n",
    "#         plt.show()\n",
    "\n",
    "#         if idx > 10:\n",
    "#             break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ca4c4",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6677.910014,
   "end_time": "2023-08-12T17:53:47.148086",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T16:02:29.238072",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
