{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../SpineNet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import spinenet\n",
    "from spinenet import SpineNet, download_example_scan\n",
    "from spinenet.io import load_dicoms_from_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r ../../SpineNet/spinenet/weights/grading/  ../output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinenet.download_weights(verbose=True, force=False)\n",
    "\n",
    "spnt = SpineNet(device='cuda:0', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPINENET_CLASSES = [\n",
    "    \"Pfirrmann\", \"Narrowing\", \"CentralCanalStenosis\", \"Spondylolisthesis\", \n",
    "    \"UpperEndplateDefect\", \"LowerEndplateDefect\", \"UpperMarrow\", \"LowerMarrow\",\n",
    "    \"ForaminalStenosisLeft\", \"ForaminalStenosisRight\", \"Herniation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    study = df['study_id'][idx]\n",
    "    series = df['series_id'][idx]\n",
    "\n",
    "    if df['orient'][idx] == \"Axial\":\n",
    "        continue\n",
    "\n",
    "    scan = load_dicoms_from_folder(\n",
    "        DATA_PATH + f\"train_images/{study}/{series}/\",\n",
    "        require_extensions=False,\n",
    "    )\n",
    "\n",
    "    vert_dicts = spnt.detect_vb(scan.volume, scan.pixel_spacing)\n",
    "\n",
    "\n",
    "    if PLOT:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        slice_idx = scan.volume.shape[2] // 2\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.imshow(scan.volume[:, :, slice_idx], cmap=\"gray\")\n",
    "        ax.set_title(f\"Slice {slice_idx+1}\")\n",
    "        ax.axis(\"off\")\n",
    "        for vert_dict in vert_dicts:\n",
    "            if slice_idx in vert_dict[\"slice_nos\"]:\n",
    "                poly_idx = int(vert_dict[\"slice_nos\"].index(slice_idx))\n",
    "                poly = np.array(vert_dict[\"polys\"][poly_idx])\n",
    "                ax.add_patch(Polygon(poly, ec=\"y\", fc=\"none\"))\n",
    "                ax.text(\n",
    "                    np.mean(poly[:, 0]),\n",
    "                    np.mean(poly[:, 1]),\n",
    "                    vert_dict[\"predicted_label\"],\n",
    "                    c=\"y\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                )\n",
    "\n",
    "        fig.suptitle(\"Detected Vertebrae (all slices)\")\n",
    "        plt.show()\n",
    "\n",
    "    ivd_dicts = spnt.get_ivds_from_vert_dicts(vert_dicts, scan.volume)\n",
    "    ivd_dicts = [ivd_dict for ivd_dict in ivd_dicts if \"T\" not in ivd_dict['level_name']]\n",
    "\n",
    "    preds = defaultdict(dict)\n",
    "    for ivd in ivd_dicts:\n",
    "        print(ivd['volume'].shape)\n",
    "        with torch.inference_mode():\n",
    "            image = torch.tensor(ivd['volume'])[None, None, :, :, :].float().cuda()\n",
    "            net_output = spnt.grading_model(image)\n",
    "            preds[ivd['level_name']] = {c: net_output[i].cpu().numpy()[0] for i, c in enumerate(SPINENET_CLASSES)}\n",
    "    all_preds.append(preds)\n",
    "\n",
    "    if PLOT:\n",
    "        for k in preds:\n",
    "            print(k)\n",
    "            for c_idx in [1, 2, 8, 9]:\n",
    "                print(f\" - {SPINENET_CLASSES[c_idx]}\", np.argmax(preds[k][SPINENET_CLASSES[c_idx]]))\n",
    "\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = pd.DataFrame(all_preds)\n",
    "\n",
    "df_ = df[df['orient'] != \"Axial\"].reset_index()\n",
    "df_save[\"study_id\"] = df_['study_id'].astype(str)\n",
    "df_save[\"series_id\"] = df_['series_id'].astype(str)\n",
    "\n",
    "df_save = df_save[[\"study_id\", \"series_id\", \"L1-L2\", \"L2-L3\", \"L3-L4\", \"L4-L5\", \"L5-S1\"]]\n",
    "df_save.columns = [\"study_id\", \"series_id\", \"l1_l2\", \"l2_l3\", \"l3_l4\", \"l4_l5\", \"l5_s1\"]\n",
    "df_save.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_save.columns[2:]:\n",
    "    df_save[col] = df_save[col].apply(lambda x: {k: x[k].tolist() for k in x} if isinstance(x, dict) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save.to_csv('../output/spinenet_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparation import *\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prepare_data_lvl2(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.read_csv('../output/spinenet_preds.csv')\n",
    "for level in LEVELS_:\n",
    "    ddf[level] = ddf[level].fillna('()').apply(eval)\n",
    "ddf = ddf.drop_duplicates(subset='study_id', keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(\"../input/train_folded_v1.csv\")\n",
    "    ddf = ddf.merge(folds, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgts = [\"spinal_canal_stenosis\", \"left_neural_foraminal_narrowing\", \"right_neural_foraminal_narrowing\", \"right_subarticular_stenosis\", \"left_subarticular_stenosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import *\n",
    "from util.metrics import disk_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tgt_ in tgts:\n",
    "    print(f'\\n-> {tgt_}\\n')\n",
    "    for level in LEVELS_:\n",
    "        print()\n",
    "        \n",
    "        for c in SPINENET_CLASSES:\n",
    "            fts = ddf[level].apply(lambda x: x[c] if isinstance(x, dict) else [0 for _ in range(len(ddf[level].values[0][c]))])\n",
    "            fts = np.array(fts.values.tolist())\n",
    "            fts = softmax(fts, -1)\n",
    "\n",
    "            tgt = y[f\"{tgt_}_{level}\"]\n",
    "            \n",
    "\n",
    "            for i in range(min(3, fts.shape[1])):\n",
    "                s = roc_auc_score(tgt == i, fts[:, i])\n",
    "                if s > 0.8:\n",
    "                    print(c, level, i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_ in tgts:\n",
    "    print(f'\\n-> {tgt_}\\n')\n",
    "    for level in LEVELS_:\n",
    "\n",
    "        fts_ = []\n",
    "        for c in SPINENET_CLASSES:\n",
    "            fts = ddf[level].apply(lambda x: x[c] if isinstance(x, dict) else [0 for _ in range(len(ddf[level].values[0][c]))])\n",
    "            fts = np.array(fts.values.tolist())\n",
    "            fts = softmax(fts, -1)\n",
    "            fts_.append(fts)\n",
    "\n",
    "        fts = np.concatenate(fts_, -1)\n",
    "\n",
    "        tgt = y[f\"{tgt_}_{level}\"].values\n",
    "        pred_oof = np.zeros((len(y), 3))\n",
    "\n",
    "        for fold in range(4):\n",
    "            model = LogisticRegression(C=0.5)\n",
    "            train_idx = ddf[ddf['fold'] != fold].index.values\n",
    "            val_idx = ddf[ddf['fold'] == fold].index.values\n",
    "\n",
    "            y_train = tgt[train_idx]\n",
    "            model.fit(fts[train_idx][y_train >= 0], y_train[y_train >= 0])\n",
    "            pred_oof[val_idx] = model.predict_proba(fts[val_idx])\n",
    "\n",
    "        s = disk_auc(tgt, pred_oof)\n",
    "        print(tgt_, level, s)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 ('default_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23df70d482b35e79876ee24f72a1adf1a99bbd330d7b589c7509f8cde77b70e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
